{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f92b4a9",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a67d2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54878a",
   "metadata": {},
   "source": [
    "# 0. Bigram Model (Chapter 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3dfddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram:\n",
    "    def __init__(self):\n",
    "        # Pseudowords\n",
    "        self.pseudo_init = '<s>'; self.pseudo_termin = '</s>'\n",
    "        self.corpus_sequence = []\n",
    "        self.token_count = {self.pseudo_init:0, self.pseudo_termin:0}\n",
    "        self.bigram_count = {self.pseudo_init:{self.pseudo_init:0, self.pseudo_termin:0}, \n",
    "                             self.pseudo_termin:{self.pseudo_init:0, self.pseudo_termin:0}}\n",
    "\n",
    "    \"\"\" \n",
    "        Preprocessing\n",
    "            - preprocess any paragraph into a sequence of tokens\n",
    "    \"\"\"\n",
    "    def preprocess_corpus(self, paragraph):\n",
    "        corpus_sequence = []\n",
    "        for sentence in (paragraph.split('.')): # Divide by sentences\n",
    "            if (sentence == '') : corpus_sequence.append(self.pseudo_termin); break # Ignore the void sentence after the last period\n",
    "            corpus_sequence.append(self.pseudo_init) # Add a pseudo_init before every sentence.\n",
    "            for word in ((sentence.strip()).split(' ')): # Remove spaces and divide by words\n",
    "                corpus_sequence.append(word) # Add all word to the sequence\n",
    "            corpus_sequence.append(self.pseudo_termin) # Add a pseudo_termin after every sentence\n",
    "        return corpus_sequence[:-1] # In case the given paragraph doesn't end with a full sentence\n",
    "    \n",
    "    def preprocess_target(self, target):\n",
    "        target_sequence = []\n",
    "        for token in target.split(' '):\n",
    "            target_sequence.append(token)\n",
    "        return target_sequence\n",
    "                \n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "        Construct Bigram Count\n",
    "            - from the corpus sequence, construct bigram count table\n",
    "            - self.corpus_sequence should have been constructed in advance\n",
    "    \"\"\"\n",
    "    def count_bigram(self):\n",
    "        prev_token = self.pseudo_init; self.token_count[self.pseudo_init] += 1;\n",
    "        for token in self.corpus_sequence[1:]:\n",
    "            if token in self.token_count:\n",
    "                self.token_count[token] += 1\n",
    "            else: \n",
    "                self.token_count[token] = 1\n",
    "                # Add a new row(of token) to the bigram_count graph\n",
    "                self.bigram_count[token] = {token:0}\n",
    "                for existing_token in self.bigram_count.keys():\n",
    "                    self.bigram_count[token][existing_token] = 0\n",
    "                    # Add a new column to the bigram_count graph\n",
    "                    self.bigram_count[existing_token][token] = 0\n",
    "            self.bigram_count[prev_token][token] += 1\n",
    "            prev_token = token\n",
    "    \n",
    "    def display_bigram_count(self):\n",
    "        display(pd.DataFrame(self.bigram_count).transpose())\n",
    "    \n",
    "    def train(self, corpus, show_table=False):\n",
    "        self.corpus_sequence = self.preprocess_corpus(corpus)\n",
    "        self.count_bigram()\n",
    "        if (show_table == True):\n",
    "            self.display_bigram_count()\n",
    "    \n",
    "    def calculate_bigram(self, target):\n",
    "        # 1. Input Processing\n",
    "        target_sequence = self.preprocess_target(target)\n",
    "        \n",
    "        # 2. Bigram Count\n",
    "        #V = len(self.token_count.keys()) # Number of tokens (including pseudowords)\n",
    "        \n",
    "        n = len(target_sequence)\n",
    "        \n",
    "        p_total = 1\n",
    "        for k in range(1,n):\n",
    "            w_k = target_sequence[k]; w_kminus1 = target_sequence[k-1]\n",
    "            p_k_given_kminus1 = self.bigram_count[w_kminus1][w_k] / self.token_count[w_kminus1]\n",
    "            p_total *= p_k_given_kminus1\n",
    "        \n",
    "        return p_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e835b01",
   "metadata": {},
   "source": [
    "## 0.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988646b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>Sam</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>and</th>\n",
       "      <th>ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       <s>  </s>  I  am  Sam  do  not  like  green  eggs  and  ham\n",
       "<s>      0     0  2   0    1   0    0     0      0     0    0    0\n",
       "</s>     2     0  0   0    0   0    0     0      0     0    0    0\n",
       "I        0     0  0   2    0   1    0     0      0     0    0    0\n",
       "am       0     1  0   0    1   0    0     0      0     0    0    0\n",
       "Sam      0     1  1   0    0   0    0     0      0     0    0    0\n",
       "do       0     0  0   0    0   0    1     0      0     0    0    0\n",
       "not      0     0  0   0    0   0    0     1      0     0    0    0\n",
       "like     0     0  0   0    0   0    0     0      1     0    0    0\n",
       "green    0     0  0   0    0   0    0     0      0     1    0    0\n",
       "eggs     0     0  0   0    0   0    0     0      0     0    1    0\n",
       "and      0     0  0   0    0   0    0     0      0     0    0    1\n",
       "ham      0     1  0   0    0   0    0     0      0     0    0    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "bigram_model = Bigram()\n",
    "bigram_model.train(corpus, show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c212e18",
   "metadata": {},
   "source": [
    "## 0.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982e1359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(I|<s>) = 0.67\n",
      "P(Sam|<s>) = 0.33\n",
      "P(am|I) = 0.67\n",
      "P(</s>|Sam) = 0.5\n",
      "P(Sam|am) = 0.5\n",
      "P(do|I) = 0.33\n"
     ]
    }
   ],
   "source": [
    "targets = [\"<s> I\", \"<s> Sam\", \"I am\",\n",
    "           \"Sam </s>\", \"am Sam\", \"I do\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(\"P(\" + target.split()[-1] + \"|\" + target.split()[0] + \") = \" + str(round(bigram_model.calculate_bigram(target),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc67c9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf6e12",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bd181",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes Classifier (Chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2151852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        import pandas as pd; import numpy as np; import math\n",
    "        self.classes = set(); self.features = set()\n",
    "        self.class_cnt = {}; self.feature_cnt = {}\n",
    "        self.data_cnt = 0\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self._count(X_train, y_train)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        probability = {}\n",
    "        for label in self.classes:\n",
    "            probability[label] = self._log_probability(X, label)\n",
    "        return max(probability, key=probability.get)\n",
    "    \n",
    "    \"\"\"\n",
    "        Count the number of..\n",
    "        \n",
    "        1. Classes throughout the whole dataset\n",
    "            - update self.class_cnt\n",
    "        \n",
    "        2. Feature occurence per class\n",
    "            - update self.feature_cnt\n",
    "    \"\"\"\n",
    "    def _count(self, X_train, y_train):\n",
    "        # 0. Validate\n",
    "        self.data_cnt = len(y_train)\n",
    "        if (len(X_train) != self.data_cnt): \n",
    "            raise Exception(\"Match the data size of X_train and y_train\")\n",
    "        \n",
    "        for i in range(self.data_cnt):\n",
    "            X = X_train[i]; y = y_train[i]\n",
    "            \n",
    "            ### [Update Class Count] ###################################################\n",
    "            \n",
    "            # 1. Check if a new class appears\n",
    "            if (y not in self.classes):\n",
    "                # 1.1 Add the class to class set and self.class_cnt\n",
    "                self.classes.add(y) \n",
    "                self.class_cnt[y] = 0\n",
    "                # 1.2 Create a column for the class in count table\n",
    "                self.feature_cnt[y] = dict(zip(list(self.features), np.zeros(len(self.features), dtype=int))) \n",
    "            \n",
    "            # 2. Update the class count\n",
    "            self.class_cnt[y] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "            \n",
    "            ### [Update Feature Count] #################################################\n",
    "            \n",
    "            for x_i in X:\n",
    "                # 1. Check if a new feature appears\n",
    "                if (x_i not in self.features):\n",
    "                    # 1.1 Add the feature to the feature set\n",
    "                    self.features.add(x_i)\n",
    "                    # 1.2 Add a row for the feature in count table\n",
    "                    for _class in self.feature_cnt.keys():\n",
    "                        self.feature_cnt[_class][x_i] = 0\n",
    "                        \n",
    "                # 2. Update the feature count\n",
    "                self.feature_cnt[y][x_i] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute P(X|y)\n",
    "        \n",
    "        1. term1: P(y)\n",
    "        2. term2: P(X|y) -> with Laplace Smoothing\n",
    "            \n",
    "    \"\"\"\n",
    "    def _log_probability(self, X, y):\n",
    "        term1 = math.log(self.class_cnt[y] / self.data_cnt, 10)\n",
    "        \n",
    "        term2 = 0\n",
    "        features_sum = sum(self.feature_cnt[y].values()) + len(self.features)\n",
    "        for x_i in X:\n",
    "            try:\n",
    "                term2 += math.log(self.feature_cnt[y][x_i] + 1, 10)\n",
    "            except:\n",
    "                term2 = 0\n",
    "        \n",
    "        return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "108ae76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[\"a\", \"b\", \"c\"], [\"a\", \"c\", \"d\"], [\"d\", \"p\", \"a\"], [\"c\", \"b\", \"b\"]]\n",
    "y_train = [\"Pass\", \"Pass\", \"Fail\", \"Neutral\"]\n",
    "\n",
    "X_test = [\"a\", \"c\", \"q\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3185ee0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m NaiveBayes()\n\u001b[1;32m      2\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 14\u001b[0m, in \u001b[0;36mNaiveBayes.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     12\u001b[0m probability \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses:\n\u001b[0;32m---> 14\u001b[0m     probability[label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(probability, key\u001b[38;5;241m=\u001b[39mprobability\u001b[38;5;241m.\u001b[39mget)\n",
      "Cell \u001b[0;32mIn[108], line 80\u001b[0m, in \u001b[0;36mNaiveBayes._log_probability\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     78\u001b[0m features_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_cnt[y]\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures) \u001b[38;5;66;03m# Laplace\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_i \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[0;32m---> 80\u001b[0m     term2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_cnt\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# Laplace\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m term1 \u001b[38;5;241m+\u001b[39m term2\n",
      "\u001b[0;31mKeyError\u001b[0m: 'q'"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayes()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1dd05d",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression Classifier (Chapter 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.classes = set()\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP] *",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
