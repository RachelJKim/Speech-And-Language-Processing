{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e83155a",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84e4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41577235",
   "metadata": {},
   "source": [
    "# 0. Bigram Model (Chapter 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83df452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram:\n",
    "    def __init__(self):\n",
    "        # Pseudowords\n",
    "        self.pseudo_init = '<s>'; self.pseudo_termin = '</s>'\n",
    "        self.corpus_sequence = []\n",
    "        self.token_count = {self.pseudo_init:0, self.pseudo_termin:0}\n",
    "        self.bigram_count = {self.pseudo_init:{self.pseudo_init:0, self.pseudo_termin:0}, \n",
    "                             self.pseudo_termin:{self.pseudo_init:0, self.pseudo_termin:0}}\n",
    "\n",
    "    \"\"\" \n",
    "        Preprocessing\n",
    "            - preprocess any paragraph into a sequence of tokens\n",
    "    \"\"\"\n",
    "    def preprocess_corpus(self, paragraph):\n",
    "        corpus_sequence = []\n",
    "        for sentence in (paragraph.split('.')): # Divide by sentences\n",
    "            if (sentence == '') : corpus_sequence.append(self.pseudo_termin); break # Ignore the void sentence after the last period\n",
    "            corpus_sequence.append(self.pseudo_init) # Add a pseudo_init before every sentence.\n",
    "            for word in ((sentence.strip()).split(' ')): # Remove spaces and divide by words\n",
    "                corpus_sequence.append(word) # Add all word to the sequence\n",
    "            corpus_sequence.append(self.pseudo_termin) # Add a pseudo_termin after every sentence\n",
    "        return corpus_sequence[:-1] # In case the given paragraph doesn't end with a full sentence\n",
    "    \n",
    "    def preprocess_target(self, target):\n",
    "        target_sequence = []\n",
    "        for token in target.split(' '):\n",
    "            target_sequence.append(token)\n",
    "        return target_sequence\n",
    "                \n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "        Construct Bigram Count\n",
    "            - from the corpus sequence, construct bigram count table\n",
    "            - self.corpus_sequence should have been constructed in advance\n",
    "    \"\"\"\n",
    "    def count_bigram(self):\n",
    "        prev_token = self.pseudo_init; self.token_count[self.pseudo_init] += 1;\n",
    "        for token in self.corpus_sequence[1:]:\n",
    "            if token in self.token_count:\n",
    "                self.token_count[token] += 1\n",
    "            else: \n",
    "                self.token_count[token] = 1\n",
    "                # Add a new row(of token) to the bigram_count graph\n",
    "                self.bigram_count[token] = {token:0}\n",
    "                for existing_token in self.bigram_count.keys():\n",
    "                    self.bigram_count[token][existing_token] = 0\n",
    "                    # Add a new column to the bigram_count graph\n",
    "                    self.bigram_count[existing_token][token] = 0\n",
    "            self.bigram_count[prev_token][token] += 1\n",
    "            prev_token = token\n",
    "    \n",
    "    def display_bigram_count(self):\n",
    "        display(pd.DataFrame(self.bigram_count).transpose())\n",
    "    \n",
    "    def train(self, corpus, show_table=False):\n",
    "        self.corpus_sequence = self.preprocess_corpus(corpus)\n",
    "        self.count_bigram()\n",
    "        if (show_table == True):\n",
    "            self.display_bigram_count()\n",
    "    \n",
    "    def calculate_bigram(self, target):\n",
    "        # 1. Input Processing\n",
    "        target_sequence = self.preprocess_target(target)\n",
    "        \n",
    "        # 2. Bigram Count\n",
    "        #V = len(self.token_count.keys()) # Number of tokens (including pseudowords)\n",
    "        \n",
    "        n = len(target_sequence)\n",
    "        \n",
    "        p_total = 1\n",
    "        for k in range(1,n):\n",
    "            w_k = target_sequence[k]; w_kminus1 = target_sequence[k-1]\n",
    "            p_k_given_kminus1 = self.bigram_count[w_kminus1][w_k] / self.token_count[w_kminus1]\n",
    "            p_total *= p_k_given_kminus1\n",
    "        \n",
    "        return p_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c914788",
   "metadata": {},
   "source": [
    "## 0.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f297243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>Sam</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>and</th>\n",
       "      <th>ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       <s>  </s>  I  am  Sam  do  not  like  green  eggs  and  ham\n",
       "<s>      0     0  2   0    1   0    0     0      0     0    0    0\n",
       "</s>     2     0  0   0    0   0    0     0      0     0    0    0\n",
       "I        0     0  0   2    0   1    0     0      0     0    0    0\n",
       "am       0     1  0   0    1   0    0     0      0     0    0    0\n",
       "Sam      0     1  1   0    0   0    0     0      0     0    0    0\n",
       "do       0     0  0   0    0   0    1     0      0     0    0    0\n",
       "not      0     0  0   0    0   0    0     1      0     0    0    0\n",
       "like     0     0  0   0    0   0    0     0      1     0    0    0\n",
       "green    0     0  0   0    0   0    0     0      0     1    0    0\n",
       "eggs     0     0  0   0    0   0    0     0      0     0    1    0\n",
       "and      0     0  0   0    0   0    0     0      0     0    0    1\n",
       "ham      0     1  0   0    0   0    0     0      0     0    0    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "bigram_model = Bigram()\n",
    "bigram_model.train(corpus, show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0021a",
   "metadata": {},
   "source": [
    "## 0.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da024486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(I|<s>) = 0.67\n",
      "P(Sam|<s>) = 0.33\n",
      "P(am|I) = 0.67\n",
      "P(</s>|Sam) = 0.5\n",
      "P(Sam|am) = 0.5\n",
      "P(do|I) = 0.33\n"
     ]
    }
   ],
   "source": [
    "targets = [\"<s> I\", \"<s> Sam\", \"I am\",\n",
    "           \"Sam </s>\", \"am Sam\", \"I do\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(\"P(\" + target.split()[-1] + \"|\" + target.split()[0] + \") = \" + str(round(bigram_model.calculate_bigram(target),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f0e2e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57678ade",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ce0f7",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes Classifier (Chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fdb61ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        import pandas as pd; import numpy as np; import math\n",
    "        self.classes = set(); self.features = set()\n",
    "        self.class_cnt = {}; self.feature_cnt = {}\n",
    "        self.data_cnt = None; self.X_shape = None;\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self._count(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # 0. Validate Input\n",
    "        if (X_test.shape[1:] != self.X_shape):\n",
    "            raise Exception(\"Match the data size of X_test\")\n",
    "        y_predict = []\n",
    "        for X in X_test:\n",
    "            y_predict.append(self._predict_single(X))\n",
    "        return y_predict\n",
    "    \n",
    "    def _predict_single(self, X):\n",
    "        probability = {}\n",
    "        for label in self.classes:\n",
    "            probability[label] = self._log_probability(X, label)\n",
    "        return max(probability, key=probability.get)\n",
    "    \n",
    "    \"\"\"\n",
    "        Count the number of..\n",
    "        \n",
    "        1. Classes throughout the whole dataset\n",
    "            - update self.class_cnt\n",
    "        \n",
    "        2. Feature occurence per class\n",
    "            - update self.feature_cnt\n",
    "    \"\"\"\n",
    "    def _count(self, X_train, y_train):\n",
    "        # 0. Validate Input\n",
    "        self.data_cnt = len(y_train);\n",
    "        if (len(X_train) != self.data_cnt): \n",
    "            raise Exception(\"Match the data size of X_train and y_train\")\n",
    "        else:\n",
    "            self.X_shape = X_train.shape[1:]\n",
    "        \n",
    "        for i in range(self.data_cnt):\n",
    "            X = X_train[i]; y = y_train[i]\n",
    "            \n",
    "            ### [Update Class Count] ###################################################\n",
    "            \n",
    "            # 1. Check if a new class appears\n",
    "            if (y not in self.classes):\n",
    "                # 1.1 Add the class to class set and self.class_cnt\n",
    "                self.classes.add(y) \n",
    "                self.class_cnt[y] = 0\n",
    "                # 1.2 Create a column for the class in count table\n",
    "                self.feature_cnt[y] = dict(zip(list(self.features), np.zeros(len(self.features), dtype=int))) \n",
    "            \n",
    "            # 2. Update the class count\n",
    "            self.class_cnt[y] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "            \n",
    "            ### [Update Feature Count] #################################################\n",
    "            \n",
    "            for x_i in X:\n",
    "                # 1. Check if a new feature appears\n",
    "                if (x_i not in self.features):\n",
    "                    # 1.1 Add the feature to the feature set\n",
    "                    self.features.add(x_i)\n",
    "                    # 1.2 Add a row for the feature in count table\n",
    "                    for _class in self.feature_cnt.keys():\n",
    "                        self.feature_cnt[_class][x_i] = 0\n",
    "                        \n",
    "                # 2. Update the feature count\n",
    "                self.feature_cnt[y][x_i] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute P(X|y)\n",
    "        \n",
    "        1. term1: P(y)\n",
    "        2. term2: P(X|y) -> with Laplace Smoothing\n",
    "            \n",
    "    \"\"\"\n",
    "    def _log_probability(self, X, y):\n",
    "        term1 = math.log(self.class_cnt[y] / self.data_cnt, 10)\n",
    "        \n",
    "        term2 = 0\n",
    "        features_sum = sum(self.feature_cnt[y].values()) + len(self.features)\n",
    "        for x_i in X:\n",
    "            try:\n",
    "                term2 += math.log(self.feature_cnt[y][x_i] + 1, 10)\n",
    "            except:\n",
    "                term2 = 0\n",
    "        \n",
    "        return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "63cf0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[\"A\", \"B\", \"C\"], [\"A\", \"B\", \"B\"], [\"D\", \"F\", \"A\"], [\"C\", \"B\", \"B\"]])\n",
    "y_train = np.array([\"Pass\", \"Pass\", \"Fail\", \"Fail\"])\n",
    "\n",
    "X_test1 = np.array([[\"A\", \"C\", \"B\"],\n",
    "                    [\"A\", \"C\", \"X\"],  # Unseen data -> smoothed by Laplace\n",
    "                    [\"F\", \"D\", \"B\"],\n",
    "                    [\"X\", \"Y\", \"Z\"]]) # Unseen data -> smoothed by Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d7050fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pass', 'Fail', 'Fail', 'Fail']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = NaiveBayes()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "classifier.predict(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdcdf98",
   "metadata": {},
   "source": [
    "# 2. Softmax Regression Classifier (Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6da8a9",
   "metadata": {},
   "source": [
    "## 2.1 Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2d346",
   "metadata": {},
   "source": [
    "* Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "25519376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBase(object):\n",
    "    def __init__(self):\n",
    "        import numpy as np;\n",
    "        self.n_labels = None\n",
    "        self.X_size = None\n",
    "        self.weights = None; self.biases = None\n",
    "    \n",
    "    \"\"\"\n",
    "        Training Data\n",
    "            - y_train: An array of one-hot encoded vectors\n",
    "            - lr: learning rate\n",
    "    \"\"\"\n",
    "    def fit(self, X_train, y_train, lr=0.1):\n",
    "        # 0. Validate input\n",
    "        if (len(X_train) != len(y_train)): raise Exception(\"Match the data shape of X_train and y_train\")\n",
    "        self.X_size = len(X_train[0])\n",
    "        self.n_labels = len(y_train[0])\n",
    "        \n",
    "        # 1. Assign random weights/biases\n",
    "        std=1e-4 # standard deviation\n",
    "        self.weights = std * np.random.randn(self.n_labels, self.X_size) # an array whose size is (n_features, n_classes)\n",
    "        self.biases = std * np.random.randn(self.n_labels) # an array of length n_classes\n",
    "        \n",
    "        # 2. Update\n",
    "        for i in range(len(y_train)): # loop over the train data\n",
    "            loss, w_jacobian, b_jacobian = self.softmax_loss(X_train[i], y_train[i])\n",
    "            self.gradient_descent(w_jacobian, b_jacobian, lr)\n",
    "            print(\"Epoch \" + str(i+1) + \"\\a:\\a (loss: \" + str(loss) + \")\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # 0. Validate input\n",
    "        if (X.shape[0] != self.X_size): raise Exception(\"Match data shape of X_test: \" + str(self.X_size))\n",
    "        \n",
    "        # 1. Predict Label(class)\n",
    "        prob = np.dot(self.weights, X) + self.biases\n",
    "        label = np.argmax(prob)\n",
    "        \n",
    "        # 2. Convert the label into a one-hot encoded vector\n",
    "        label_vector = np.zeros(self.n_labels, dtype=int); label_vector[label] = 1\n",
    "        return label_vector\n",
    "        \n",
    "        \n",
    "        \n",
    "    def softmax_loss(self, X, y):\n",
    "        \"\"\"\n",
    "            Compute the Loss and Gradients for a binary classifier\n",
    "            Return:\n",
    "                - Loss\n",
    "                - Gradient\n",
    "        \"\"\"\n",
    "        # 1. Compute the summ for softmax\n",
    "        summ = 0\n",
    "        for k in range(self.n_labels): # loop through the classes\n",
    "            w_k = self.weights[k]; b_k = self.biases[k]\n",
    "            summ += np.exp(np.dot(w_k, X) + b_k)\n",
    "        \n",
    "        # 2. Update the gradient matrixes, and calculate the loss\n",
    "        loss = 0; \n",
    "        w_jacobian = np.zeros(self.weights.shape); b_jacobian = np.zeros(self.biases.shape)\n",
    "        for k in range(self.n_labels):\n",
    "            w_k = self.weights[k]; b_k = self.biases[k]\n",
    "            softmax = (np.dot(w_k, X) + b_k) / summ # softmax\n",
    "            gradient = softmax - y[k]\n",
    "            for i in range(self.X_size):\n",
    "                w_jacobian[k][i] = gradient * X[i]\n",
    "            b_jacobian[k] = gradient\n",
    "            \n",
    "            # Loss -= softmax for the correct class\n",
    "            if (y[k] == 1): loss -= softmax;\n",
    "        \n",
    "        return loss, w_jacobian, b_jacobian\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, w_jacobian, b_jacobian, lr):\n",
    "        \"\"\"\n",
    "            Update the weights and biases\n",
    "        \"\"\"\n",
    "        self.weights -= lr * w_jacobian\n",
    "        self.biases -= lr * b_jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26872211",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea9e5b",
   "metadata": {},
   "source": [
    "### 1) Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "b2227bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEzCAYAAABQRpQZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABEDklEQVR4nO2dd3xUVfr/3yd1ShKKoSlFFBXbKkXUBSkKFgRRUEFXf4quuCoWXNzVr7prWXXtu65lxYKFthZs2FYUUVhRAyggqICAAtJrepnn98fJJJnMnclkMmEmk+ed17wgN+ee+8zMnc+c8hQjIiiKoiQLKfE2QFEUJZaoqCmKklSoqCmKklSoqCmKklSoqCmKklSoqCmKklTUKWrGmMOMMd/UeOwxxtywD2xTFEWpN6Y+fmrGmFRgA3C8iKxrNKsURVGipL7Tz1OA1SpoiqIkKvUVtTHA9MYwRFEUJRZEPP00xmQAG4EjRWSzw9/HAeMAvF5vr+7du8fSTkVRFBYuXLhNRNqEa1MfURsBXCMip9bVtnfv3pKXlxeZlYqiKBFijFkoIr3DtanP9PMCdOqpKEqCE5GoGWO8wBBgZuOaoyiK0jDSImkkIgXAfo1si6IoSoPRiAJFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZIKFTVFUZKKiETNGNPSGPOaMeZ7Y8wKY8yJjW2YoihKNKRF2O6fwAcicq4xJgPwNKJNiqIoUVPnSM0Y0wLoDzwHICKlIrKrke1SlIiYxzx605t00mlPex7kQXz44m2WEkcimX52BbYCk40xi40xzxpjvI1sl6LUySIWcRqnsZCFlFPOZjZzB3dwMzfH2zQljkQiamlAT+ApEekBFEDwXWOMGWeMyTPG5G3dujXGZipKMHdxF0UUBRwrpJDHeZx88uNklRJvIhG19cB6Efmy8vfXsCIXgIhMEpHeItK7TZs2sbRRURz5lm8RJOh4Gmn8zM9xsEhJBOoUNRHZBPxijDms8tApwPJGtUpRIuAojsJggo6XUUYnOsXBIiURiNRP7VpgqjFmCXAscG+jWaREzUY2chmX0Za2HMRBPMRDlFMeb7Majb/wF9y4A4558HAlV5JNdpysUuKNEQkevjeU3r17S15eXsz7TXaWspSXeZlCChnFKAYy0HEk4sQudnE4h7ONbVVClkkmx3Ec7/AOLWnZiJbHj0/4hOu4juUspwUtuIEbuI3bSCU13qYpjYAxZqGI9A7XJlI/NaWR+Sf/5BZuoZRSfPh4gRcYyUhe5EUAvud7CinkGI4hzeFte4Zn2M3ugJFZCSXMYx7tac+TPMllXNaoz8GHjxJKgkZPjcnJnMwyluHDR4oGyChomFRCsIlN3MzNFFFEBRUIQgEFzGQmL/Myh3M4venNIAbRlrbMYlZQH5/zedBOoJ8SShjPeFawolHsL6OMm7iJHHLIJptudOMDPmiUa4VCBU3xo3dCAvABHziOvgop5Cqu4kd+pJBC9rKXnexkNKNZxaqAtt3pTjrpIa9RRlnVqM+PX0AbylVcxRM8QQEFVFDBalYzilF8yZd1n6woMUZFLQFw4XJcOzMYyigLEp4yyniGZwKOXc3VZJAR8hrllLOLXQAsYxn96EcGGbhxM5ax7GVvVLbvZCdTmBI0SiyiiL/xt6j6VJSGoKKWAJzJmY6hPWmVP7Upo4z1rA84diAH8iEf0pWujtfw4uUczmETm+hLX+Yzv2oNbDrTGcrQqGxfz3oyyQw6Lgjf831UfSpKQ1BRSwCyyeY1XsODh2yy8eLFhYuJTAw5PXyd17mRGymhpOpYX/qymtXcxE24cVeN/rLIYjCDGcIQJjGJUkoD+iqhhMWVP2CnpZ/xGe/xHnvYE9b2rnSljLKg4ymk0ItegHU1mchEetOb0Ywmj/rtjK9jHSMYgQsXOeRwDddoxIASGhGJ+aNXr16i1J/dslumyTR5Xp6XDbJBRESulqvFK17B4cctbhkpIx37mifz5DK5TMbIGHlD3pAKqRARkXPkHMe+siVbpst0+Ua+kfbSXrIlW3IkR9zilkkyKazdt8gt4hFPQH9e8coyWSZrZa20ltaSIRmCIEaMeMQjb8qbEb0mu2SXtJE2kiIpVX1nSqacKCeKT3z1eHWVZADIkzr0R0UtwfGJT2bIDOkgHRzFyCUuWStrI+7vXrlX3OIO6scjHlkki6SNtHEUz8WyOKyN/5J/SSfpJB7xyAAZIHmSJyIiF8vFkiqpQX22l/ZVQisiUi7l8ra8LdfJdXKv3CvrZb2IiDwmjwUJpl80F8iC6F5UpckSiajp9DPBMRhGM5qDOMjx75lkspKVEfd3BVfgxh3gAuHCRT/6sYUtFFMcdE4ppUEbE7VtHM94fuZnCijgUz6tmnrOZjYVVASds5vdbGADYKe/AxnIhVzIYzzGndzJoRzKbGazkIUUUuh43e/4LuLnrTQfVNSaCL3o5eiyUUIJh3GYwxnO5JLLV3zFUIbiwkVLWnI1V/MWb7Gb3Y7nVFDBdrZHZXcuuY7HffhoQQsAnuVZFrGoap2shBIKKWQMYziKo/A45CQ1mHo9b6X5oKLWRJjABFy4Ao65cTOCEfUO3j6Yg3mHdyiiiJ3s5GEexoWLAQxwXPT34uVszo7K7olMDBKlTDIZxjByyAGoCg2rTSml9KBHkMtLBhkcyqH8lt9GZZOS3KioNREO5EDmMY/+9CeNNFrSkhu4gZd5OWbXaEc7bud2PHiqRMSLlx70YBSjqtoJQiGFEWWYvZiLuZ7rceGiBS2qxPN5nq9qE8q/zoePXHJZwAIGMYgUUsggg9GM5mM+jjguVmleaEB7krOl8qcb3YJGeqGYy1wmMYld7GI0oxnDmCrheY3XmMAEfuVXPHj4I3/kdm6vM0xpJztZznI60pEudAn421SmciVXUkBBwPEudGENa6rEy4cPU/mjNE8iCWhXUUtgfuEXXuEV8slnGMOqFt8jIZ98LuZi3ud9MshAEO7hHq7juqjt+ZAPGcnIgKmiBw83cAP3cE/U/frwcSmX8hqvAdVOx3OYwzEcE3W/SvKhotaEmcEMLuMyfPgoowwXLi7mYp7iqYhGKudyLu/ybsBuphcv05nOcIZHZdMJnOAYz+nFy3a2O0YW1IdlLGMuc2lLW4YzPOKRpdJ8UFFrouxmNx3oEBRP6cXLW7zFKZwS9vwd7GB/9g+INvDzW37LfObXy57v+Z4lLOFKrqyKH62JGzerWMX+7F+vfhWlvkQiarpRkID8l/86xnwWUMA0ptV5/ja2hczYsZGNEdtRSilncRY96ckVXBEyZCqDDNqgdSmUxECTRCYgoRbdDSaivGFd6eqY+TWVVAYxKGI77uIuZjObosofJzx4+Ct/DZv2SFH2JTpSS0BO5VRHL3w3bi7iojrPTyedh3gowD8sjTSyyeZ2bo/Yjmd4xlHMDAYXLg7iIJ7kSSYwIeI+FaWxUVFLQLLJZhrTcOPGg6cq79kf+AMDGBBRH7/n97zJm5zCKRzCIYxlLN/wTcjURE6ECk8SBB8+trOdzWyOKNFkGWVMYxqjGMXlXK4JJJVGQzcKEpitbOU1XqOAAs7kTA7n8H16/VGM4k3eDOtk68HD/dzPeMaHbFNGGYMZzEIWUkABKaTgwsU93MMN3NAIlivJiu5+KlGxjnVV62kb2UgKKUE52GqSRRZrWct+7Of49+lM5wquCHKudeFiAxtoTeuY2q8kL1pNqpmQTz5llNGKVg3uax3rOJZj2cveqnW9dNI5mqNZytKQ1z+SI1nMYjrQAbBT1P/yX2Yzm4/5OEjQwO6afsqnjGRkg+1WFD+6ptaE2cIWhjKU1rSmHe04mqNZxKIG9Xkv95JPfsBGRRllrGUth3JoyPO2s527uAuAVayiDW04ndN5iIeqMurWRpCqoPZoqaDCUTCV5ouKWhNFEAYykI/4iLLKn2UsYyAD2cSmqPv9hE9CVnW/nutDRg2UU84sZuHDR3/6R5SqyIWLgQyMys5SSpnABHLIoQUt6EY3/st/o+pLSS5U1Joon/M5v/BLkACVUcZzPBd1vx3p6Hi8lFLO5VymMS1kmFYrWrGABWxjW8j+XbjIJps2tOFDPnR0Mo6EcYzjaZ6mkMKqsnzncE696x8oyYeKWhNlDWscXSmKKW5QFadbuMUx/9lpnEZb2jKSkZzGaUHOtv7A9nAjtBRSuJzLmclMNrKRHvSIysbtbOc//MexLF9DAuuV5EBFrYnSgx6OrhZevA1Knngqp/Ioj1ZVW88kk6EMZSpTq9pMYQo96YkHT1WOtMu5nLGM5bf8NuRILoUUbuVWBjM46hEawM/87JiDTbQsn4KKWpPlN/yGkzkZN+6qY2mk0YpWXMzFDep7HOPYyla+5EvWs56ZzCSLrKq/78d+LGABX/EVM5jBGtbwGI9hMOzHftzBHUFhWgbDvdxbtTvaEA7mYEcXk1RS6U3Y3X6lGaCi1oSZyUxu4RY60pFccrmUS8kjL0CAoiWDDA7n8JA1BgCO5EhO53Ta0z7g+C3cwgd8QD/6sT/7M5jBfMmX3MRNDbYLIIccruXaoGmyGze3cmtMrqE0XSJyvjXGrAX2AhVAeV3Ob+p8qzQ2gvA4j/MgD7Kd7ZzACTzMwxzLsfE2TWlEYu18O0hEQm9rKUnDEpZwB3ewkIUcwiHczu0Rx5w2JmWUMZOZvMu7tKUtv+f3XMu18TZLSTA0okAJII88BjCAIooQhJ/5mS/4gqlMjbqiVCzw1wZdxjLyySeNNJ7kSV7gBUYxio/4iLWspSc9OY7jtI5BMybS6ecaYCcgwNMiMilce51+NgxB2MIW3Lgb7HFfXwYxiE/5NOh4ZzqzlrVxE4uneZobuTEoc4gXL7nksoMdlFNOCikcz/G8y7uaDjwJiWXm234i0hM4A7jGGNPf4WLjjDF5xpi8rVu3RmGuAtap9hAOoQtdaEMbhjEsrDNrMcW8z/u8x3shUwXVh1DOqxvZWFVs2E855bzO61zKpUxkYqO6U0xnuuPzK6aYX/iFveyliCIKKOB//I/7uK/RbFESHBGp1wO4A5gYrk2vXr1EqT+rZbV4xSvU+EmXdOklvcQnvqD2H8qHklPjJ0uy5C15q0E2HCwHB1zf/+MVr5RJWVW7UimVATKgyt40SRO3uGWKTGnQ9UNxppzpaFeon47SsVHsUOILkCd1aFSdIzVjjNcYk+3/P3AqsKwRdbbZ8iRPBvlflVHG93zPIhZRTjnf8R0b2ch2tnMO57Cnxk8++YxhDL/ya9Q23MzNePEGHPPg4SquCnCYncY08sirCiYvp5wiihjHuJiMGGtzFVcF2RWOcKmSlOQmkulnO2CeMeZb4CvgXRH5oHHNap58z/eUURZ0PJVUpjGNtrTlBE7gIA7iRE50DJMShBnMiNqGy7mcMYwJWDvrRjfu5M6AdtOZ7pgdw2CYxrSIsuHWh6EM5WquxoWLLLKq4kedMoekkx5QUV5pXtQpaiLyk4gcU/k4UkQ0uK6RGMCAgAgBP8UU8xRPsZOd5JNPCSWsZnVATU8/pZSym91R27CUpUxneoAorWQl4xgX0C7UqKmAAsYzniM4glWsitqO2hgMD/AAP/ADT/EUM5jBBjbwKq/SghZVr1sWWXSkI3dzd8yurTQtNPNtArGTnRzBEWxjW1X2jXTSaU97NrAhbFptPx48fMInHM/xUdnwO37HDGYEXSuTTNaxjna0A2wZv5GMDJnLzGDoRCfWsCaiClgNYTvbeYmX+JEfOYETOJ/zHb8clKaP1v1sYrSiFTOYETD18+ELKWgZZAS4LXjxMopR9KFP1DYsY5njtVy4WMvaqt9P5VSu4zoyyXQsjycIO9nJZ3wWtS2Rsh/7MYEJPMVTXMIlKmjNHBW1BOMu7grIkVZBBT58jv5hKaTwIi9yPudzLucynem8yIsN8iU7lmMdR1YllNCNbgHH7uVeVrKSYzgmZH9b2BK1LYoSDRpRkGB8xmchNwDSSKsSPC9eJjCB8yt/YsHHfMzrvB40UvPgYSxjHQurdKITV3EV13Fd0FS0jDL60jcmtilKpOhILcGoa+rUkY6cyIm8wAtVNQFiwW52M4IRjmtk13Itj/FYyHMv5EK60jXAdi9eruEaDuCAmNmoKJGgI7UE4zIu42medtzZLKccL17+x/9ift23eMtx2ppBBl68YRf7XbhYwAKe5Ele4RVyyOEaruEczom5nYpSFypqCcbf+Ts/8AMf4OwKuJKVlFASsgBKtOxlr2PBlTLK2MOeOs/34uWmyh9FiSc6/UwwXLh4n/dpS1vHv7txO6ayBlvRfTGLg2I0I+E0TnMcqXnwMJzh9e5PUeKFilqCcjM3O2Z2vYZrgsSniCJGM5pOdGIgA2lLW+7gjnp59XejG+MZjxdvVf9evAxjGCdxUsOfkKLsI3T6maBcz/VsZCOP8zgZZFBCCRdyIX/jb0FtxzOed3iHksofgId4iAM5kEu5NOx1BGEhC9nIRiYwgaEMZTKTKaWUC7mQMzlTc5MpTQqNKEhw9rCHNayhE51oTeugvxdRRCtaVYlZTbrTnRWsCNn3r/zKEIawjnWkkEIJJVzN1TzMwypkSkKiEQVJQA45HMMxjoIGdoE/FFsJn9duFKP4nu/JJ5897KGEEiYxielMb5DNihJPVNSaOG1o4+gUazBh18LWs57FLKaCioDjBRTwT/4ZczsbG0HIJz+i+FgluVFRa+IYDI/zeMCmQiqpZJHFvdwb8ry97A2qzelnF7tibWZUlFPONKZxFmcxhjHMZrZju8lMpgMdaEUr9mM/HuCBmKc+UpoQdWWRjOahmW/3Pf+T/8kwGSbdpbtcJpfJKlkVtn25lEuu5AZljM2QDPmz/HkfWR3evlPl1IBMwF7xyi1yS0C7V+QV8YgnKEvv/XJ/nCxXGhMiyHyrGwXNmHd4hzGMoYQSKqjAjZu2tGUhCx2ntPvatgu5MMjnzoWLH/iBznQG7GbID/wQdH5LWrKd7Y2e9kjZt+hGQRNiN7u5ndvpTnd605sXeKHR14eGM5yv+IoruILTOZ17uIclLIm7oAHMYpajE3EqqXzMx1W//8Ivjufnk08RRY1mn5K4qJ9aAlBIIX3owzrWVblmjGc885nPMzzTqNc+kiN5iqca9RpO7GEPZZSFFNBWtArISuInhRRa0KLq9+50ZxGLgs7PJTfIeVlpHuhILQGYylQ2sCHA16yAAqYwhZ/4KY6WxZ5f+ZXBDKYNbdif/TmKo1jIwqB2YxnrmHwylVSGMrTq9/u5PyiziQcPf+fv6mvXTFFRSwBmM9sx5U866SxgQRwsahx8+BjAAOYyl9LKn+/4jkEMYjObA9oexmE8wzN48JBT+ZNLLh/wQUC238EM5h3eoRe98OChO915kRe5hEv29dNTEgSdfiYAXehCOumOlaT2Z/+gYz58vMd7zGQm2WRzGZeFzT4bC1aykgIKOIqjAkrl1YdP+ZRNbAqaUpZRxvM8zy3cEnD8d/yOEYzgcz7HhYuTOMnx2qdwSsgizErzQ0dqCcCVXBk01UohhVxy6U//gOM+fJzN2YxhDJOZzOM8zomcyJM82Si2rWY1R3EUx3Is/elPO9oxi1lR9bWWtY6bH8UU8yM/Op6TRRZncAaDGBS1mCrNCxW1BOBgDuZN3qQd7fDixYWLHvRgDnOCXBLe4R0+4ZOq6aoPH0UU8Uf+yHa21+u6JZSwnvUhC/9WUMHJnMwKVlBIIXvZyw52MJrRrGRlvZ9nT3o6OsV68WrabyVmqKglCEMYwkY28jVf8wM/kEceXegS1O5VXg25/lbT1SEcPnzcxm3sx34cxmHkkst93BckOHOZy052Bo2u/DGi9eVYjmUgAwMW9tNJJ5dcLuTCevenKE6oqCUQKaRwOIdXOZY6kUVWSIfSSF0Y7ud+HuVRCiioGoH9jb8FCdUmNjmOrCqoiFhAa/MGb3Art9KFLrSjHZdzOV/zdZDtFVTwEi8xgAH0pz8v8EJQnKqiOKERBU2Mr/iKQQyikMKA4y1owSY2BewMOiEI+7EfO9kZ9LeOdAxwZl3HOg7mYEcxySCDHewIWam9IQjCSEbyER9VjUq9eDmZk0PWUlCaBxpRkIT0oQ93cAcuXGSRVeXuMItZdQoa2BFQqID12m4VXegS0jnWhYvFLK63/ZHwBV8ECBpYv71P+IT5zG+UayrJg4paE+QmbmINa3iKp3iRF9nEJvrRL6Jz00jjIA5y/NvRHB107HiOd2xbTjn7sR/P8Azd6EYOOQxhCN/wTcTPIxSf8qljNa0iiviUTxvcv5LcqKg1UdrTnou4iLM5u85aobV5lEeD1rA8eHiYh4PaTmBCUNtUUjmMw/gP/2ECE1jNavayl9nMph/9WM7y+j+hGuSS61gty4WLXHIb1LeS/KioNUOGM5xZzKIf/WhLWwYxiI/4iIEMDGo7iEHcz/1Vnv0ZZNCCFvShD/dzf9BObBFF3M3dDbLvPM5zzPWWQopjNfpd7OJGbuQADuBADuQu7nIc6SnNg4g3CowxqUAesEFEhoVrqxsFycc2tjGAAaxhDUUUOQab+zmYg1nFqgZdbz7zGcnIqkwbLlzMZGbQNLuUUn7Db1jDmip/OzduTuAEPuZj3VRIMiLZKKiPi/b1wAogp0FWKU2SV3iFtaytEplQggZwCIc0+Hp96ctGNrKIRQhCL3o5jt5mMpMNbAhwIC6iiK/4igUs4ERObLAtStMioumnMaYjcCbwbOOaoyQqU5ka5EbihBs3t3FbTK6ZSirHcRx96BMy9fgXfOGYd62cco0HbaZEuqb2D+BPEDproTFmnDEmzxiTt3Vr+CpGStMjlD9aOum4cJFOOp3pzH/4zz4NeTqYgx03SjLIcIzIUJKfOkXNGDMM2CIiwUmvaiAik0Skt4j0btOmTcwMVBKDq7jKUdj2Z392s5utbGUtaxnO8H1q10VcRAYZAcdSSaUFLQLyrinNh0hGan2Bs4wxa4EZwMnGmCmNapWScJzN2YxlLC5cePGSTTZtaMMsZlXtiMZjUb41rZnLXI7maDIqf07gBOYxT7N6NFPqFSZljBkITNTdz+bLT/zEZ3xGLrmcxmmO2WnjxRa2kEZayMLPStMn1rufisJBlT+JSFvaxtsEJQGol6iJyKegcSqKoiQuGlGgKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSoaKmKEpSkVZXA2OMC/gMyKxs/5qI/LWxDVMUJQJ274Znn4VPP4Vu3WD8eDj44LrPq6iADz+E1avhmGPgpJPAmEY3d19Qp6gBJcDJIpJvjEkH5hlj3heRBY1sm6Io4di8GXr1gh07oKgI0tPhmWfgzTehrAx+/hmOOw569gw879dfoW9f2LbNtktLg6OOgtmzweuNy1OJJXVOP8WSX/lreuVDGtWqJsSPP8L779v7p6F8+y2cfz4cfjhccAEsW9bwPpUk5s47YcsWK2hgBaqgAE4/HUaPhgkT7AjsjDOgtLT6vMsug19+gb17obgY8vNh8WK44464PI2YIyJ1PoBU4BsgH7g/RJtxQB6Q17lzZ0l29u4VGTxYxO0WadFCxOUSufBCkbKy6Pr77DMRj0ckJUUE7L8ej8gXX0Tex44dIuPHi7RpI9K+vcjw4SJ9+ogccYTIrbeK7NwZnW1KgtKxo71Z6nq43SJ//7s9p6hIJD3duV2bNvF9PhEA5EldelVXg4DG0BKYAxwVrl2vXr321XOMG5dcIpKZGXhPeDwif/tbdP395jfO91mfPpGdX1IicuihIhkZzv1kZoocdJAVYz8+nxXNSZNEPvnE/q5ESVGRyPz5IsuW7bsXsnv3yEQN7JsvIlJQIJKW5tymVat9Y3cDiETU6rX7KSK7KkXt9FiMEpsqZWUwYwaUlAQeLyyExx+vf38isHSp898WLYqsjzffhI0bA2cZNSkpgU2bYOJEuP9+a/9JJ8HgwXDDDXDWWXZZZdu2+tvf5KiogP/+F/79b/jyS/sGNIQpU6BtWzvNO/54OPJIWLMmNraGY/x48Hgia+u/WT0e6NMneFMgPR1GjoytffGiLtUD2gAtK//vBj4HhoU7J9lHagUFIqmpzl92Xm90febkOPeXmxvZ+RMnRvaFnZpqv6jT00WMCfxberrIyJHR2d9k2LhRpFs3kexsOy3zekUGDBApLIyuv0WLbD81X8iUFDsyauwRW0WFyKWX2rWPnBz7XGpPH8AO32+8sfq8FStEWre2UwsQycoS6dpVZMuWxrU3BhCjkVoHYI4xZgnwNfCRiMxqHIltGng8cMQRwcdTUuCUU6Lr87rrgr90PR47ioqEQw6JbOOqogLKy+1os/YApawM3nnH/pu0jB0La9faRfKiIruw/uWXcPfd0fX3xBPBQ3afzy7gL2hkB4GUFJg8GVassP/OmQPz5kF2Nrhctk1WFnTpArfdVn1e9+7w00/w4INw/fXw5JOwfDm0adO49u4r6lK9aB7JPlITsWtRXm/1mmtmpkjLliI//hhdf2VlIldeWf2l63LZRf/y8sjO373bfvnWHn3V95GaKlJcHN1ziDu7doksXWpfDCcKCkIvkrdvH901zzjDub+cHJG3347+uTSEzZtF7r/f3lAvvWTX+5IEYr1REOmjOYiaiMiqVSLXXisyaJDILbfYmU1D2b5dZOHC6HYqly+3Gwvp6XaKmZ1txTErKzJBM0akb9+GP4d9Tnm5/QZwuaqf9IQJdnpWkz17Qota69aRX2/LFrshUFws8sQT1dO4mg+Xq3o6V15u/1/X1vju3SIPPSRy2mki48bZaygBqKg1U3btsp9fEZG1a0WWLBH53e+cP8/+tUGPx25+rVgRX9uj4o47goXF46l2Y6hJr17Oi4mXX173dfbsETnrLDssz862j8ceEzn88MB1Na/X2uTziTz6qB3C+8+5+27ntbZt20Q6d67uJzXVPod4jfYSFBW1KNi1S2TePCsGycSWLXbt2j9qy8oSaddO5N577ef5kUfsKLFJ0rKl8+jLye9q6VLb3uWSqnWDTp0iWyQfPtzZj+eNN0QeeMAOk08/XeTdd237Z591FtsHHgju+6abnP1x2rSJfA2iGaCiVg98PpHbb7f3ut+ZdsiQ0MszTQmfz05p331XZPJk64j78svRb/glFD5f6IXE1FTn9ldcYefnqalWpLxe6/0cjs2bnXcWwToIFhQEn9Oli3P71q2DR2uHHOLcNi1N5OGHrSNiY7Bsmcjo0XZHePhwkS+/bJzrxAgVtXowdaq9t2veT5mZIiNG2L8XF1tRePllkZUr42pqvVizRuSww+zIzC/WDz0Ub6tizNFHOwvCMccEt33//eA3Guzcu7Q09DWWLrXTx1CLkQceKLJ1a+A5oTyhjQleXzvuOOe2/uns0UcHek7Hgq+/tn37w1iMsSPJDz+M7XViiIpaPTj2WOf7KTNTZNYsez/X9E1r3z7hv9TE57MhUv57tuYM6OOPI+9n3jyRsWNFzjtP5NVXE3A29OGHzqO1/fcPFo9zz3V+o3NyRGbPDn2N4uLQouZfl7vmmsBzjjnGue2BBwb3P2WKs9jW3Hi4444Gv1QB9OvnfK1u3WJ7nRiiolYPDjjA+f31eJw3t/x/i8WOZ2Px7behPyf+EWhd3H23fZ5+zfB6RYYODd5YjCtTpgQ7wIIVobfeCmx71lmhRe3998Nf54knnK9T85uuJh995Lym9tprwX37fCI33BB6dAd2ihpLQnmQp6Qk7NpEJKKmSSIrGTIEUlODj6elWV9KJ0pKbCqrRGXnTmu/E1u21H3+hg3wt7/Z8C8Re6ygAD77DN57L3Z2NpgVK6ozVdSksND+rSYXXeQcWlRRAf37h7/O1VfDSy+Fzjvmd3j1M3iwfaF++1to1cqmCXr9dRg1KvhcY+DRR2H+fMjIcO4/MzO8ffVhwwb7nJ1ISYnttfYxKmqV3HkntGhRfT8ZY+/9Cy4ILWoVFbBy5b6zsb707u0cHeB2Rxbm9/HHNiSwNvn58MYbDbcvZhx5pLMQiMChhwYeMybwDU1JsWL04ouRxVGeey6cfHLwN6DbDePGBbcfMMAK1Y4dkJdn0wKFo3dvm3sqpdZH0+OBP/yhbvsi5ccfg0XYT5s2wddvQjRdy2NM5842qPzaa6FHD/tlOmcO3HJL6HNcLujXb9/ZuG2bjWy55BIbOL9nT/j2Xi888oj9PPgHF243dOoEV15Z9/Wys50HJamp0LJlvc1vPAYMcI7kF7HhUH7mzrUvXnFx9bHUVDj7bOfRUyheegm6drUhSOnp9kVKSbHD4vLy8OeK2Bvtq69Cx6O99hq0a2ffALfbvoGnnRbZmxYp3bo5HzfGZjdoytQ1P43m0RTX1EJRVCRy3XXB69DG2B37/Px9Y8fy5da9yr+k4/FYP7OVK62LU8uWdjlm8GDbtibz54uMGSMycKD1BY10E62w0O6Y1l5ycbsTzNn9jTdCh02cfXZ1u8GDndu4XNZBsT7k59uNiJrrUh6PyDnnhD5n+XKRgw+2C5PZ2XbH9Z13nNuWllrH26eeEvnmG3ts40aR996zO7Gx4LzzgtcIvd6E3t5HNwqix++35vFUJz9o377a8/7qq4N38BuTvn2DhTU1VaRDh8D70hi75r1hQ2yuO3++fb45OdWJLSZNik3fMePjj513JlNSRC67rLrdgQc6i1pWlsj339fvmpMnO+/CuN3OolNaKtK2bfCb6PGIrF4d/lo+nw0Dy8y0b4THYx19t22rn81+li2z9n/wgc3ekZVl7erTR+Srr6Lrcx+hotYAHn44cmfwxqakJNgtI9wjM9PGoobD57ODgK5d7T198sk2i06o63/wgR0QJWT23LIyKxhO29M1/W7OPdf5hfR667/bd8klzi++1yvy3HPB7WfNcs4vlZ4u8n//F/5azzwTfDOmp9vohfpQWmpHkv6US9nZduS4fn2TyRCqotYAnD4jEHl+s1hSXh46DjvUY8iQ8H36R6G1P4/ffbdvnlPM+fZbO5TOzq5Oc/KvfwW2WbYseHQVbbriu++uDrWq+cjOtq4ctZk82bk92Jxo4TjyyNDfXvWJbXvwweDpZmqqzSfXRFBRawChRkbGxOdL7fzzg12YMjOdXY0yMkT+9KfQfeXnO7tbpaTYiJkmS3m5yJw51jct1JBy8WKr+Dk51u9r8uTo3tANG4LX8VJT7dDXyYnvzTedb6jUVJHp08NfK1QtAre7fkHK3bo595OR0WQCfyMRtUhK5DVLjjjCuZrT4YfHpzziU0/BDz/YMo3+u7F3b+vJMG9e4IZeZqbN9ByKNWuc/dd8Pli4MPa27zNSU2HgwPBtjj3WpvJuKPvvb/u5+GLr8+V/Q6ZPd3aHmD7duZ+KCpsCPBxDh8LzzwfvrLZqZbeyI8XJlw+svTVvoCaOunSE4B//sLvpNXG77fF40Lq1rWL2/vvwr39Z74S5c+Gtt2zFM7fb3pu//S18/nn4e/2AA0LXMiguDu2TqdTixBOto+LKlbbk3Lx5oV/4cN8Wc+eGv84dd9gbwO9Xlppq3Tyee65+/mSjRjn783XqBB06RN5PolPXUC6aRzJMP0Xszt/gwXaH8ZRTbAxkouLz1S906bLLnKfYLpdNR6TEmHbtnKd+ENmC/7Ztdh1v4ECRYcNE7rmn/snvtm2z02P/umJmpp1C16cOY5whgumnse1iS+/evSUvLy/m/Sqxweezo7sXX3T+e4cOtjKVEkNatAjtLd2pU2TVsDdssNPrTZvs7xUVMHw4TJvmHOPnRGGhbT93ri1s8fvf26l0E8EYs1BEeodro2tqzZApU+DVV0P/fffufWdLzFm8GL7+2oaIhArojQeHH24LvDhx1FGR9XHBBXZBtOb6wKxZNrzk+usj68PjsUL2+99H1r4JomtqzZAnnrBf2E4YY+uBNjnKymDYMBu3NmECnH8+HHxwZCOgfcF99zkLbHq6XTOri23bbGhV7QXPwkK7i+Tn++/tLtGZZ8LDDzfxb6joUFFrhuTnh/6bx2M/C02Of/wDPvnEfsgLC23M5/r1cOGF8bbMMmiQzQKw337Vx9q2tVk8+vSp+/yiotCbAnv3wl/+AiecAEcfbYs0v/ce3H67/X3r1tg8hyaCiloTY88euyTy7LN2wy0aRo8OnaDhtNPsTKnJMWlSsMtCRYXNjJEoZeeHD7e2+Iuvbt5s0xNFQseO0L598PG0NNvnAw/Y6W15efVorqjIXuO++2L3HJoAKmrAkiW2CLHHYxfJ//73xHRr+Ogju6Z75ZV2CeXQQ+Hee+vfz4QJNsmE0xf/zJlw6qmh0y0lLLULCvsxJrT/SrxISan/Wp8x8PLLNvWKP9eZ12v7KS0N/fxLS+HNNxtkblOj2Yva6tXQt6+duRQV2Y2lu+8O77waDwoK4Jxz7L/5+XaGVVwM99xj18XrQ3a29RsNNZv5+GPrC9ekOP98Zx+sLl2Sxwerb1+7ZvbnP9tp9UMPRSbYLVo0vm0JRLMXtQceCHamLiyEF15IrKWIDz5wFqHi4tCuGeGoqAidFResWDYpbrvNClhWlv3d5bLq/fLL8QkBaSw6drQZTadOtUP2UOsIfjyeyHdGk4RmL2pff+2c1y8z0yYHTRRCzS58vtDRL+Ho3Ln68+/Ejh317zOutGxp1xGefNJmoL3zTli1Co47Lt6WNR7G2DAtp9TbGRn2+IAB8PTT9g2/+GI7NUl26vLOjebRlCIKLr44tGd9IhVV2brVOcmD12vTAkXDP/4R2sk9Jye29iuNRH6+yKBBNttITo4Ncj/+eJH//EfkrrsCU7Gkptqsn3Xlb0tg0MIrdfOnPwWP4N1uGDEi9FJMcbEdEBx3nE3nnpFhsy/fc0/jbTDk5tq6HG63nTYaY9eJR4ywC/vR8Ic/hJ69JLFvZnLh9doF4QULbCxoXp79/1ln2bWVmg6JFRV2Qfbuu+Nn776gLtUDOgFzgOXAd8D1dZ3TlEZqIiJz51bXx3S7Ra691pZ5XLpU5JFHRJ59VmTHDtu2uFikZ0/n1D0ej8jll9f/+nv2RF6Ae/lykZtvtjZ+8olz1pyCApsM9osv6o4HnTEjcASYmmoz1MS6bq6yj1m+PHSd0oMOird1UUMs8qkBHYCelf/PBn4Ejgh3TlMTNT/FxVYEfD6Rq66ywpWZaad4Xq+tdfvCC3XXnN20KbLrffaZSPfuImlp9jr/7/81XEz8NXFzcmys8gEHiCxZEv6cJUvs8x0xQuTppxO25KNSH7ZtszeV0006cGC8rYuamIha0AnwFjAkXJumKmp+3nvPWbhatLAf/HAZZ1u0sGJVF99/H5x5NjNT5LTTorf7u++cR5Bt24ps3izy+usi775rxbs+lJWJvPSStW3ECFsrpIlkf27enHde8EKsx2Mr2jdRIhG1egW0G2MOBHoAISJzk4MXXrD+YLURsSGGqamh186Ki+Ggg4KPL11q0221b2/D8h55JHhHs6TEFgpevdqGLfopL7f25OSE90545hlnt6U9e2wiiMxMe74xNg46kvJ+Pp/NUfi//1W/JrNnw+WXwz//Wff5zZa9e+Gbb+xia+3ao/uKF16w6VjefNMuxKanW9+2aBdhmwp1qZ7/AWQBC4GRIf4+DsgD8jp37rwPtTv2jBzpPArLybFp751GQ/6p53nnBfZVXm5TZHs89rzsbDty6tkz9EjPn+K+vNyun3m9tkZBhw7hMz9feGH4UWTt5xLJNPPdd52rz7lcCV1JLb488IB9s1u0qN6N3LIlfvbs3GnfrNLS+NkQI4jV9BNIBz4EboykfVOffs6c6fxB9nrtIvwLL1SXzvO7g7hctj5o7andpEnBU1ljRFq3di6m4nJVl7e78UbnilahXDimTw+/3lfzkZ0t8tprdb8W11zjfL7bLfLvfzfsdU5KZs0KfhPS022NQ6XBRCJqdbp0GGMM8BywQkQeaYzRYqIxYoSdInq9dqqWmWldKaZOtQ7al1wCixbZau4PPmhnGoWFdjpW2w/y6aeDp7Ii1mHW5QqMEvB4bMqs/fe3f3/qqeAUQYWFoTPVjBoFv/mN7cdPqKgBkfDZOvzk5jpHH6Wl2RT5Si0eeST4DS8rs+m8162Lj03NjbpUD+gHCLAE+KbyMTTcOU19pCZiF8LnzRO59VZbWWz9+uq/3XKLXdTPyLA7l263daFw4uijnUc6Xq/dkDj7bDsV7NjRzlrKy+15P/8cPEqrufAfiuJiWyby5JNt33fdFXr6WPM5heKnn5yn2y1a2FGrUoujjgo9NF64MN7WNXnQEnmx58MPnT/kxjjXzPz7353bd+gQ3oestNQKh9Pn49RTI7e3okJk+PDqGZExVizvvjvyPl5/3Qqjv0p7mzaBNYKVGvzpT8G1DP2iVt9tZyUIFbVG4JxznIUG7OJ/bQoKRHr1qh4tuVxWYObOrftajz7qXHv3668D261YITJmjEiXLtYFafbswL9XVNjq6mPGWOfg+fPr/7yLimy/n39ePZpUHNiyRWT//at9xPzfIs8/H2/LkgIVtUbg1FNDi5oxzqOvsjK7KH/11SL33Ve/mNKXXxY57DAriv37Bxf+WbbM/q1m/KrHIzJ1asOep9IAtm0Tuf12keOOs9+Cn38eP1sqKkSefNJ6eXfoYL/V/DtRTZBIRE2rSdWTyZOt648TmZl2gX9fZro56yzrc1b7bWzTxuaGq09ZSCUJueoqeOml6h2ntDRbQ3T58sDU4k2ESKpJ6S1fT373O+dA97Q0m7evpqDt3GljjZcvj6zvigormiecAD172gD2ugpnL1gQLGhgd2Q3b47suknD55/DRRdZpX/5Zbvr2Bj88gt88UXiFzXZsME64NbcQi8vt97Y//533MxqdOoaykXzSObpp4jIrl02Jrj2OvDSpdVt7rzTrp+1aGGng7162VClcIwZE7iG5naLnHBC+DWsULurLlczi+G89177QhsjVdvLJ50UW4fTvXtFzjij+o11uURuuy1xY8ZmzbK7O043yODB8bYuKtA1tcZh506RVq2qPz9gs1t07Gg/Q2+8EbzAn5Ym0q9f6D6/+cbZhSMrS+Stt0KfN2NG8Hlut8gVV8T6WScwmzeHTjY3Y0bsrnPeecFB4l6vXfhMRJYscb6pjBG56KJ4WxcVkYiaTj+jYPp0G6dZc9pXUWFnI+++6+x/WV5us+yGqgA1b55zsZP8fFszIBSjR8Ndd9kstllZdl1v9OgmWGOgIcyda+Maa1NQYMvSxYI9e+Dtt4MDdgsKbN6yeOLz2enwhx8GVoE/+mhbKLm2B7aIfV1WrNi3du4jVNQioKIicG1r1SrnYsClpdZpPFRtg4qK0Gmy27Vz/ly6XDYtfTj++Ed7za+/tpsDkyc7Z3hOWkJF+qemxi7sYffu0Lsu8SxmsWIFHHigrW14/vk2Y0LN4sbvv++cCbSw0N44SYiKWhiKi21VqexsGzJ15JE2i0afPs75/dPS7AJ/t27O/ZWX2/t/3rzgz8GwYc7hSKmpdu27Llwu6N7dpupvdpx8srOKZ2baegWx4IADrHjWJiXF1leMBz6fzbixfr3dGdqzx26/T5xoq7mDjZlzKmIhYke4SYiKWg1E7Axj2DAYMgROOgmef97eEz6f3cU84ww47DB7j9cUIZcLjj3WpvNxuvfBDiaGDrX9d+5sd9v9KYxcLvj0U5tyyOOxotmunXXXSJYKb41Gerqt+deunf0GysmxwbqPPAI9esTmGikpNoe7x1M9KkxPt+Xn4pUee948O4Ksvf1dXFy9u5me7jwFgNA3ahNHRa0G111n3TLefdfmDMvLC/6SKymBf/zDulJcdZUd7R9wgP1y/Ogje78fcojzqEvEehns3m3vu5desp87P0cdBStX2uvOm2f/v327XcNLlCLjCcuxx1oXhnfesSXsf/3VlpCLJSNHwpw5dqrXsqV9s1NSbCK7UOW+GkJBgV1LuPVWeOWV4GR5u3Y5T7t9Ptiyxf4/NdVmYKg9BfV4bEaGZKSunYRoHk1x9/PHH5030JwedT29cMHotR8HHODcx3//W52WOzvb2vbUU7F/3ko92bVLJDc3cOvb5apfQG4krF5tMxf4t9GzsmzxiG3bqtts2xZ617dmWFZhoQ0ArumKMnZsk4x3Q106Iuff/45MiNLSbD7/uvj4Y5H27e29GE4sPZ7gc3fvds6L5nbbehpKnPD5bHBtqDfy229jd62BA4NrN6anB1f2efDBQP88v1OkU/D8mjX2xkyk2o/1JBJR0+lnJa1b25F6XbjdcNNNdbc7+WQ7G1qwAL77zi7i18YY55Tas2Y5b7SVldkpqxInXn/dRi04kZIC334bm+sUF9vr1PbxKSuD114LPDZxot3hPP98u2nw6KN27cJp4+TAA+2NmeSLtCpqlQwb5ixqaWnQtq1dghgyxObq79o1sj5TUuyO6UEH2XVbj6f6GmlpdjPg4YeDzyssdPZZKy+PLLGjgl1TeukluyBZ03erITz+eOjiFD6f87b322/bLfHcXBg82Prd1IW/kIQTTt92/fvDjBnWT23cuNDFXJsLdQ3lonk0xemniE3p066dXcPyr2W9+Wbs+l+2zFaE79nTTmF/+sm53S+/hF4q+eST2NmTtDz5pH0Bs7Lsm+jx2BJYDaVHj9DrCIceGhwuNXmycz72SJLRnXaaDVOpeW5mpsj48Q1/Hk0YdE2t/pSX23xjc+bEN6ffPffY+9+/rOL1ilxwQeRhhvPmWQEdPtyWt0uCmhuRsWJF6ErT/orU0XLffc7fNunpIr/+arOEfvONTfdTUWE3FJwEcNCguq+1fr1I585WlNPSrMClpIgcfLAtkpGo8aaNjIpaE+err2zhk8sus8VWIr2Pa68de7027nT+fJvaK9Jq8E2S226zIuAURPvCCw3re+9ekSOOqN7F8edyf/RRKzZer71Ou3Z2iB+qmHCrVpFdr7RU5JFHgiv0eDxWYJshkYhavep+KvuW446zj/qwdSvcdlug21RBAcyfbx3fMzLscs3Uqba4TNJRVOS87uXz1Z3HqS6ysqwT4bRp8N57Nn5t7Fi7VrZ9e3W7/Hzr8BgqrKquuDc/6el2w6C8PPB4YSHccw/ccIOunzmgGwUR4PPZ6lFffhl8fyUan33mvPElYj/Te/ZY59/zz4eff9739jUqPp8VnlAOqWec0fBruN22kvPrr9vyYatWOVeQ9vlsPF3N0l5gf//rXyO/Xl5ecMSAn/XrI++nGaGiVgcLF9qQpgED7O5nu3Y22iBRyckJ/RmoSXk5vPhi49uzzxCx9QUffNB567hHD1umPtZs3uycjLK42GbJuPpqK2Qulw2uf+QRW8swUsIFErdrF53NSY6KWhgKC+3MYsMGO6PYu9dm2RgxwkbhxJKdO2OT7GHQoMhmJKWlSZYZ98svbXybU/oUsD5kzz4b++v26+c8zczKst+CDz5ob5q1a+0bXN/Qrdtvdx7tXXqpjXNVglBRC8NbbzlPNysqYMqU2Fxj3Tr7uWjf3i61/OY3sGRJ9P2lpQXGdjtlEwF7/PTTo79OwvHJJ+HXzAoLrWNqrDnmGJs+3OutPuZ220Be/6JlZqZ9QyLx7q7NoEHW386fQcHjscL42GOxsT8J0Y2CMGzd6ixqJSU2b1lDKSuzgvbrr9Vr20uXWl/KNWuiTwXmj+3+/HM7upw5E159tTpxpcdjNyBiscSUMLRubcUj1EgNbAB4YzBliq2J8PTTdgh88cXwhz9EJ2JOjBplg+l377biGSrrhgKoqIVl4EDnNeesLDstjYZff7XRBcuW2ZHUrl3Bm3VlZfZz0pAkCqmp1n6w0RLDhtlkEsXFNj/bJZfE7jOXEJx3ng0ZCkVaWuOpeGqqnQ72729TqxxxRP12JXfssFPjr7+2I7wrr7RD95oYEz5ZXn4+vPmmvaGGDLH5sZordfl8RPNIJj+1iy4KDC73eKzvZLjq6qFYvNj6Uvrdl2q7H9V8XHdd9XlFRSI332wro2dnWyfc9etj9hSThzlzRFq3Dvbid7nsi/fLL3X38emnIsOG2eiBP/+57mo5IjYLxtChgVkwLrwwMo/ntWutk67fYdjlsuEs9QmOnzfP3hj+7Alut408SEIHXdT5tuFUVNjCwAMHivTtK/L009F75/fqFVrEavuJ1qwXMmRIoCN7aqr179y9OzbPMakoK7Mexi+9ZNPr9O8v8pe/2MrpdfHMM4GCmJlpX+hNm5zbFxSILFxov/lqRxq43SJ//Wvd1xw5MjgbB4gcf3xkz7e01Aq5U0zdrFmR9dGEUFFLIIqKgkP5nB7p6bYiu9/rf/Fi55RIHo/IY4/F9SklF0VFdrRT+4XOyBCZODG4/cMP2zfB6Rz/Ize37us65ZgCK3SRxOnNmRO6DN4559T7ZUh0IhE13f3cR6SmRr6GNW6cTbBaVmZ3Qp08BgoLbVojJUaEqqxUWmqjB2ry9tvW1aKw0O7EhCKSlCqh1t7S0oKrQDkRrmBzY2TjbQKoqO0j0tPhnHOc03zXpKwMbr7Zevx36ODsrA72s3DkkbG3s9mSmxtaIGov2t9/f/hdVrAL+/37133d3/8+WNgyMuDccyP7Fuzb19lur9fuwjZH6hrKAc8DW4BldbX1P3T66czOnSLHHWdnHJGm+27d2lZhr72pkJMT2Rq2Ug/69w9+ob1ekffeC2zXtWv4Ny0jw75BkaQpLiqyaYbcbjuV9XrtetquXZHZfNVV9nq11zCGDWuS6brrglisqQH9gZ4qarHB57N52558MvhedHrk5Ij85z8i555r79XUVCuMS5bE+5kkIdu22XQmbrd94T0emyWjNuPGOWcCyciw50+cGNlOa02WLhWZNs3eHJHuWn71lfO3Y0aGyA8/1O/6TYRIRK3OSbuIfGaMObDRhorNDGOgd2/7mD3bLteEc4S33yvWeba01DoD146aUWLEfvtZj+WffrIxZEcf7RyScdttNq323r3VUz+Px2bGHTs2umsfdZR91IdZs5xvHmNsiu9DD43OliaOrqnFkSlT4He/s0sqqanOjr5lZTatPFRHySiNzEEHwYknho4x69TJ7uBceaV1tD39dBt3Gq2gRYv/xqlNaqoN1WqmGPEPBcI1siO1WSIS8qvEGDMOGAfQuXPnXuvWrYuVjUlPSYlNCXTBBXZHs6DA7ni6XLZO7o03xttCJSFZs8aKau3Rmtttg4rbtImPXY2IMWahiPQO1yZmYVIiMgmYBNC7d++6lVKpIjPT3n8ffABvvGGnmi1b2o2xPn3ibZ2SsHTtamPu/vCH6nhQf7aFJBS0SInZSK0mvXv3lry8vAaapihKROzYYdfQUlNh6FCbVC9JiclIzRgzHRgI5Bpj1gN/FZHnYmOioigNpnVruzirABGImohcsC8MURRFiQW6+6koSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlKhoqYoSlIRkagZY043xvxgjFlljLm5sY1SFEWJljpFzRiTCjwBnAEcAVxgjDmisQ1TFEWJhkhGan2AVSLyk4iUAjOAEY1rlqIoSnREImoHAL/U+H195TFFUZSEIy1WHRljxgHjKn8tMcYsi1XfMSYX2BZvI8Kg9jUMta9hJLp9h9XVIBJR2wB0qvF7x8pjAYjIJGASgDEmT0R6R2jkPiWRbQO1r6GofQ2jKdhXV5tIpp9fA4cYY7oaYzKAMcDbDTVOURSlMahzpCYi5caY8cCHQCrwvIh81+iWKYqiREFEa2oi8h7wXj36nRSdOfuERLYN1L6GovY1jCZvnxGRfWGIoijKPkHDpBRFSSpiKmqJHE5ljHneGLMlUV1NjDGdjDFzjDHLjTHfGWOuj7dNNTHGuIwxXxljvq20785421QbY0yqMWaxMWZWvG2pjTFmrTFmqTHmm0h28PY1xpiWxpjXjDHfG2NWGGNOjLdNfowxh1W+bv7HHmPMDSHbx2r6WRlO9SMwBOug+zVwgYgsj8kFGogxpj+QD7wkIkfF257aGGM6AB1EZJExJhtYCJydQK+fAbwikm+MSQfmAdeLyII4m1aFMeZGoDeQIyLD4m1PTYwxa4HeIpKQPmDGmBeBz0Xk2UovB4+I7IqzWUFU6swG4HgRWefUJpYjtYQOpxKRz4Ad8bYjFCLyq4gsqvz/XmAFCRS5IZb8yl/TKx8JsyBrjOkInAk8G29bmhrGmBZAf+A5ABEpTURBq+QUYHUoQYPYipqGU8UIY8yBQA/gyzibEkDl9O4bYAvwkYgkkn3/AP4E+OJsRygE+K8xZmFl9E0i0RXYCkyunL4/a4zxxtuoEIwBpodroBsFCYYxJgt4HbhBRPbE256aiEiFiByLjSrpY4xJiGm8MWYYsEVEFsbbljD0E5Ge2Gw311QuhyQKaUBP4CkR6QEUAAm1Jg5QOS0+C3g1XLtYilpE4VRKaCrXql4HporIzHjbE4rKqckc4PQ4m+KnL3BW5brVDOBkY8yU+JoUiIhsqPx3C/AGdrkmUVgPrK8x8n4NK3KJxhnAIhHZHK5RLEVNw6kaQOVC/HPAChF5JN721MYY08YY07Ly/27shtD3cTWqEhG5RUQ6isiB2PvuExG5KM5mVWGM8VZu/lA5rTsVSJhdeBHZBPxijPEHi58CJMQGVS0uoI6pJ8QwS0eih1MZY6YDA4FcY8x64K8i8lx8rQqgL3AxsLRy3Qrg/yqjORKBDsCLlbtPKcArIpJwrhMJSjvgDfu9RRowTUQ+iK9JQVwLTK0ckPwEjI2zPQFUfhkMAa6ss61GFCiKkkzoRoGiKEmFipqiKEmFipqiKEmFipqiKEmFipqiKEmFipqiKEmFipqiKEmFipqiKEnF/wfLQ85gapENWwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = []; y_train = []\n",
    "\n",
    "# Create random data points\n",
    "data_per_label = 50\n",
    "labels = [[0,0,1], [0,1,0], [1,0,0]]; centroids = [(2,2), (3,5), (5,2)]\n",
    "for i in range(len(labels)):\n",
    "    xs = centroids[i][0] + (np.random.random_sample((data_per_label,)) - 0.5) * 2\n",
    "    ys = centroids[i][1] + (np.random.random_sample((data_per_label,)) - 0.5) * 2\n",
    "    X_train += list(zip(xs, ys))\n",
    "    y_train += [labels[i] for j in range(data_per_label)]\n",
    "    \n",
    "# Shuffle\n",
    "shuffle = list(zip(X_train, y_train)); np.random.shuffle(shuffle)\n",
    "X_train = list(map(lambda pair: pair[0], shuffle))\n",
    "y_train = list(map(lambda pair: pair[1], shuffle))\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.scatter([point[0] for point in X_train], [point[1] for point in X_train], c=y_train, label=y_train)\n",
    "plt.xlim([0, 7]); plt.ylim([0, 7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e4302",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "57b6fe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\u0007:\u0007 (loss: 0.0002834079303161162)\n",
      "Epoch 2\u0007:\u0007 (loss: 1.2531157251782442e-06)\n",
      "Epoch 3\u0007:\u0007 (loss: 7.693763284653816e-06)\n",
      "Epoch 4\u0007:\u0007 (loss: -0.10698618150369432)\n",
      "Epoch 5\u0007:\u0007 (loss: -0.02918202168229454)\n",
      "Epoch 6\u0007:\u0007 (loss: -0.014788694253845592)\n",
      "Epoch 7\u0007:\u0007 (loss: -0.005689175707771049)\n",
      "Epoch 8\u0007:\u0007 (loss: -0.00043694982073096296)\n",
      "Epoch 9\u0007:\u0007 (loss: -0.00017098441015582543)\n",
      "Epoch 10\u0007:\u0007 (loss: -1.214404084058401e-05)\n",
      "Epoch 11\u0007:\u0007 (loss: -2.8860404803386552e-08)\n",
      "Epoch 12\u0007:\u0007 (loss: -2.850025341734751e-09)\n",
      "Epoch 13\u0007:\u0007 (loss: -8.964244966040522e-06)\n",
      "Epoch 14\u0007:\u0007 (loss: -6.519148682947382e-11)\n",
      "Epoch 15\u0007:\u0007 (loss: -1.71990702601842e-07)\n",
      "Epoch 16\u0007:\u0007 (loss: -7.240151347301707e-08)\n",
      "Epoch 17\u0007:\u0007 (loss: -9.155697666942596e-12)\n",
      "Epoch 18\u0007:\u0007 (loss: -1.0922992405290316e-09)\n",
      "Epoch 19\u0007:\u0007 (loss: -6.633108628386953e-14)\n",
      "Epoch 20\u0007:\u0007 (loss: -2.9847508202235633e-07)\n",
      "Epoch 21\u0007:\u0007 (loss: -3.1121066301212043e-09)\n",
      "Epoch 22\u0007:\u0007 (loss: -1.0035741772487248e-09)\n",
      "Epoch 23\u0007:\u0007 (loss: -1.1105164952716803e-06)\n",
      "Epoch 24\u0007:\u0007 (loss: -4.3771931238994506e-08)\n",
      "Epoch 25\u0007:\u0007 (loss: -2.2856827680462537e-08)\n",
      "Epoch 26\u0007:\u0007 (loss: -1.1801153090248202e-15)\n",
      "Epoch 27\u0007:\u0007 (loss: -3.0179707972014234e-12)\n",
      "Epoch 28\u0007:\u0007 (loss: -1.555035068477582e-18)\n",
      "Epoch 29\u0007:\u0007 (loss: -4.756894766042108e-15)\n",
      "Epoch 30\u0007:\u0007 (loss: -5.774933696662126e-11)\n",
      "Epoch 31\u0007:\u0007 (loss: -3.2535285325377178e-12)\n",
      "Epoch 32\u0007:\u0007 (loss: -9.268250398235617e-08)\n",
      "Epoch 33\u0007:\u0007 (loss: -7.548238540293986e-18)\n",
      "Epoch 34\u0007:\u0007 (loss: -4.49419446943057e-16)\n",
      "Epoch 35\u0007:\u0007 (loss: -6.2783654674760294e-18)\n",
      "Epoch 36\u0007:\u0007 (loss: -1.0603225833845519e-16)\n",
      "Epoch 37\u0007:\u0007 (loss: -4.294608519189496e-09)\n",
      "Epoch 38\u0007:\u0007 (loss: -1.7893176027046148e-18)\n",
      "Epoch 39\u0007:\u0007 (loss: -2.3070519025364603e-07)\n",
      "Epoch 40\u0007:\u0007 (loss: -6.5501096285862e-19)\n",
      "Epoch 41\u0007:\u0007 (loss: -2.8801039859335404e-24)\n",
      "Epoch 42\u0007:\u0007 (loss: -5.169092897616761e-22)\n",
      "Epoch 43\u0007:\u0007 (loss: -7.128547963352872e-30)\n",
      "Epoch 44\u0007:\u0007 (loss: -6.91761087512835e-12)\n",
      "Epoch 45\u0007:\u0007 (loss: -4.5284541802168947e-10)\n",
      "Epoch 46\u0007:\u0007 (loss: -1.072850926242553e-09)\n",
      "Epoch 47\u0007:\u0007 (loss: -1.2280551751227133e-11)\n",
      "Epoch 48\u0007:\u0007 (loss: -2.3342527022507528e-11)\n",
      "Epoch 49\u0007:\u0007 (loss: -2.5960825186966637e-11)\n",
      "Epoch 50\u0007:\u0007 (loss: -7.732899452626606e-15)\n",
      "Epoch 51\u0007:\u0007 (loss: -8.323320164801728e-21)\n",
      "Epoch 52\u0007:\u0007 (loss: -3.046535498303759e-17)\n",
      "Epoch 53\u0007:\u0007 (loss: -2.932613020404751e-18)\n",
      "Epoch 54\u0007:\u0007 (loss: -1.1071610243996298e-21)\n",
      "Epoch 55\u0007:\u0007 (loss: -8.190452964908858e-16)\n",
      "Epoch 56\u0007:\u0007 (loss: -1.0039090230120265e-23)\n",
      "Epoch 57\u0007:\u0007 (loss: -5.299594569447936e-23)\n",
      "Epoch 58\u0007:\u0007 (loss: -2.605038194595178e-30)\n",
      "Epoch 59\u0007:\u0007 (loss: -7.389867101307902e-27)\n",
      "Epoch 60\u0007:\u0007 (loss: -1.0089362989758792e-31)\n",
      "Epoch 61\u0007:\u0007 (loss: -5.70452317023462e-19)\n",
      "Epoch 62\u0007:\u0007 (loss: -2.2413559220613304e-30)\n",
      "Epoch 63\u0007:\u0007 (loss: -4.2789867353645495e-15)\n",
      "Epoch 64\u0007:\u0007 (loss: -4.382953269425872e-33)\n",
      "Epoch 65\u0007:\u0007 (loss: -4.377450283367009e-32)\n",
      "Epoch 66\u0007:\u0007 (loss: -4.2779673133594836e-30)\n",
      "Epoch 67\u0007:\u0007 (loss: -3.282961292466413e-30)\n",
      "Epoch 68\u0007:\u0007 (loss: -3.272601184877585e-27)\n",
      "Epoch 69\u0007:\u0007 (loss: -2.833994705180577e-22)\n",
      "Epoch 70\u0007:\u0007 (loss: -2.0136666478930124e-19)\n",
      "Epoch 71\u0007:\u0007 (loss: -3.0154292270123245e-34)\n",
      "Epoch 72\u0007:\u0007 (loss: -4.1236419865423944e-20)\n",
      "Epoch 73\u0007:\u0007 (loss: -1.7116900158143711e-34)\n",
      "Epoch 74\u0007:\u0007 (loss: -1.0200676061076526e-31)\n",
      "Epoch 75\u0007:\u0007 (loss: -1.949049684598398e-42)\n",
      "Epoch 76\u0007:\u0007 (loss: -8.682138783842665e-24)\n",
      "Epoch 77\u0007:\u0007 (loss: -3.5863251424984593e-31)\n",
      "Epoch 78\u0007:\u0007 (loss: -4.61450310591315e-30)\n",
      "Epoch 79\u0007:\u0007 (loss: -1.7969892118254666e-34)\n",
      "Epoch 80\u0007:\u0007 (loss: -5.7169716630341704e-33)\n",
      "Epoch 81\u0007:\u0007 (loss: -2.5871211333688923e-42)\n",
      "Epoch 82\u0007:\u0007 (loss: -1.2406030402816953e-15)\n",
      "Epoch 83\u0007:\u0007 (loss: -1.5482876668945204e-20)\n",
      "Epoch 84\u0007:\u0007 (loss: -1.0768041430562983e-37)\n",
      "Epoch 85\u0007:\u0007 (loss: -2.556584185795411e-47)\n",
      "Epoch 86\u0007:\u0007 (loss: -6.14563643084887e-44)\n",
      "Epoch 87\u0007:\u0007 (loss: -2.13657038665474e-23)\n",
      "Epoch 88\u0007:\u0007 (loss: -2.5182932738411278e-34)\n",
      "Epoch 89\u0007:\u0007 (loss: -1.0393579404510528e-30)\n",
      "Epoch 90\u0007:\u0007 (loss: -3.7000044938471656e-32)\n",
      "Epoch 91\u0007:\u0007 (loss: -3.1320900874734355e-48)\n",
      "Epoch 92\u0007:\u0007 (loss: -3.617350178947976e-32)\n",
      "Epoch 93\u0007:\u0007 (loss: -3.066712615870261e-25)\n",
      "Epoch 94\u0007:\u0007 (loss: -1.7622429858553388e-38)\n",
      "Epoch 95\u0007:\u0007 (loss: -2.913267751171635e-45)\n",
      "Epoch 96\u0007:\u0007 (loss: -9.986611270825407e-22)\n",
      "Epoch 97\u0007:\u0007 (loss: -1.8338598031793574e-53)\n",
      "Epoch 98\u0007:\u0007 (loss: -4.765352377056074e-33)\n",
      "Epoch 99\u0007:\u0007 (loss: -5.931115561070624e-42)\n",
      "Epoch 100\u0007:\u0007 (loss: -3.2318893618788684e-31)\n",
      "Epoch 101\u0007:\u0007 (loss: -9.085674952043902e-51)\n",
      "Epoch 102\u0007:\u0007 (loss: -5.611248468185434e-44)\n",
      "Epoch 103\u0007:\u0007 (loss: -7.574591733530572e-24)\n",
      "Epoch 104\u0007:\u0007 (loss: -1.2953759073830251e-40)\n",
      "Epoch 105\u0007:\u0007 (loss: -4.807035695139843e-44)\n",
      "Epoch 106\u0007:\u0007 (loss: -4.212742842296791e-37)\n",
      "Epoch 107\u0007:\u0007 (loss: -2.5888218359920014e-54)\n",
      "Epoch 108\u0007:\u0007 (loss: -1.2853833400674115e-48)\n",
      "Epoch 109\u0007:\u0007 (loss: -2.033523784744844e-38)\n",
      "Epoch 110\u0007:\u0007 (loss: -1.7670510975616453e-51)\n",
      "Epoch 111\u0007:\u0007 (loss: -2.060578894231268e-31)\n",
      "Epoch 112\u0007:\u0007 (loss: -1.6881660264291236e-21)\n",
      "Epoch 113\u0007:\u0007 (loss: -8.64003977645097e-51)\n",
      "Epoch 114\u0007:\u0007 (loss: -6.063799149374386e-24)\n",
      "Epoch 115\u0007:\u0007 (loss: -9.102673302927377e-28)\n",
      "Epoch 116\u0007:\u0007 (loss: -4.7234323637846404e-33)\n",
      "Epoch 117\u0007:\u0007 (loss: -2.0042966733916978e-58)\n",
      "Epoch 118\u0007:\u0007 (loss: -1.233197842769191e-32)\n",
      "Epoch 119\u0007:\u0007 (loss: -2.496900529808974e-25)\n",
      "Epoch 120\u0007:\u0007 (loss: -7.938178150219102e-46)\n",
      "Epoch 121\u0007:\u0007 (loss: -3.925911298728034e-35)\n",
      "Epoch 122\u0007:\u0007 (loss: -6.101681011646567e-47)\n",
      "Epoch 123\u0007:\u0007 (loss: -6.993962042383528e-37)\n",
      "Epoch 124\u0007:\u0007 (loss: -1.0897007594600274e-25)\n",
      "Epoch 125\u0007:\u0007 (loss: -3.4622106197255117e-60)\n",
      "Epoch 126\u0007:\u0007 (loss: -7.95249389664414e-48)\n",
      "Epoch 127\u0007:\u0007 (loss: -3.0858428849674308e-52)\n",
      "Epoch 128\u0007:\u0007 (loss: -1.2300558263431399e-64)\n",
      "Epoch 129\u0007:\u0007 (loss: -4.481229261610411e-22)\n",
      "Epoch 130\u0007:\u0007 (loss: -3.6769395948263135e-58)\n",
      "Epoch 131\u0007:\u0007 (loss: -1.787685011177691e-23)\n",
      "Epoch 132\u0007:\u0007 (loss: -1.3570102844239742e-51)\n",
      "Epoch 133\u0007:\u0007 (loss: -4.68858593784465e-56)\n",
      "Epoch 134\u0007:\u0007 (loss: -4.837460931729727e-63)\n",
      "Epoch 135\u0007:\u0007 (loss: -8.63417944236009e-62)\n",
      "Epoch 136\u0007:\u0007 (loss: -1.5394235388288986e-69)\n",
      "Epoch 137\u0007:\u0007 (loss: -1.5013628636376582e-44)\n",
      "Epoch 138\u0007:\u0007 (loss: -5.067740640675576e-25)\n",
      "Epoch 139\u0007:\u0007 (loss: -3.715897568122569e-81)\n",
      "Epoch 140\u0007:\u0007 (loss: -8.06401853924974e-73)\n",
      "Epoch 141\u0007:\u0007 (loss: -1.9243043609571176e-63)\n",
      "Epoch 142\u0007:\u0007 (loss: -6.026121901008981e-69)\n",
      "Epoch 143\u0007:\u0007 (loss: -1.9611415107658211e-63)\n",
      "Epoch 144\u0007:\u0007 (loss: -9.069212209443831e-57)\n",
      "Epoch 145\u0007:\u0007 (loss: -6.43522357891958e-33)\n",
      "Epoch 146\u0007:\u0007 (loss: -2.2523585390656827e-71)\n",
      "Epoch 147\u0007:\u0007 (loss: -1.0029113791152418e-60)\n",
      "Epoch 148\u0007:\u0007 (loss: -3.3142881964778956e-38)\n",
      "Epoch 149\u0007:\u0007 (loss: -2.141734729295713e-59)\n",
      "Epoch 150\u0007:\u0007 (loss: -6.60044621275513e-64)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionBase()\n",
    "classifier.fit(X_train, y_train, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1226df",
   "metadata": {},
   "source": [
    "Here, we can see that the training is not done well.\n",
    "The loss doesn't converge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca8af1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311fb44c",
   "metadata": {},
   "source": [
    "## 2.2 (Update) Mini-Batch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612cd48",
   "metadata": {},
   "source": [
    "- Mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "586b45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression1(LogisticRegressionBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, X_train, y_train, batch_size=1, lr=0.1):\n",
    "        # 0. Validate input\n",
    "        if (len(X_train) != len(y_train)): raise Exception(\"Match the data shape of X_train and y_train\")\n",
    "        self.X_size = len(X_train[0])\n",
    "        self.n_labels = len(y_train[0])\n",
    "        \n",
    "        \n",
    "        # 1. Assign random weights/biases\n",
    "        std=1e-4 # standard deviation\n",
    "        self.weights = std * np.random.randn(self.n_labels, self.X_size) # an array whose size is (n_features, n_classes)\n",
    "        self.biases = std * np.random.randn(self.n_labels) # an array of length n_classes\n",
    "        \n",
    "        # 2. Update \n",
    "        n_batches = int(np.ceil(len(y_train) / batch_size)); current_batch_size = batch_size\n",
    "        for batch_no in range(n_batches): # loop over the train batches\n",
    "            loss_batch = 0; \n",
    "            w_jacobian_batch = np.zeros(self.weights.shape);  b_jacobian_batch = np.zeros(self.biases.shape)\n",
    "            if (batch_no == n_batches): current_batch_size = len(y_train) % batch_size # size of the last batch\n",
    "            for trial in range(current_batch_size):\n",
    "                i = batch_no * current_batch_size + trial\n",
    "                loss, w_jacobian, b_jacobian = self.softmax_loss(X_train[i], y_train[i])\n",
    "                loss_batch -= loss; w_jacobian_batch += w_jacobian; b_jacobian_batch += b_jacobian\n",
    "            # average over the batch\n",
    "            loss_batch /= current_batch_size; w_jacobian_batch /= current_batch_size; b_jacobian_batch /= current_batch_size\n",
    "            self.gradient_descent(w_jacobian_batch, b_jacobian_batch, lr)\n",
    "            print(\"Epoch \" + str(batch_no+1) + \"\\a:\\a (loss: \" + str(loss_batch) + \")\")\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "e3742277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\u0007:\u0007 (loss: 8.514504610370409e-05)\n",
      "Epoch 2\u0007:\u0007 (loss: 0.11127259613146578)\n",
      "Epoch 3\u0007:\u0007 (loss: 0.07848582234655482)\n",
      "Epoch 4\u0007:\u0007 (loss: 0.07894768190362436)\n",
      "Epoch 5\u0007:\u0007 (loss: 0.060160057921417945)\n",
      "Epoch 6\u0007:\u0007 (loss: 0.03421109348090191)\n",
      "Epoch 7\u0007:\u0007 (loss: 0.026924093477718864)\n",
      "Epoch 8\u0007:\u0007 (loss: 0.012378412833668759)\n",
      "Epoch 9\u0007:\u0007 (loss: 0.012265343299041958)\n",
      "Epoch 10\u0007:\u0007 (loss: 0.005552302134646888)\n",
      "Epoch 11\u0007:\u0007 (loss: 0.002442081516210052)\n",
      "Epoch 12\u0007:\u0007 (loss: 0.0044252317247359275)\n",
      "Epoch 13\u0007:\u0007 (loss: 0.002584115195366767)\n",
      "Epoch 14\u0007:\u0007 (loss: 0.0021081605803733427)\n",
      "Epoch 15\u0007:\u0007 (loss: 0.0003958373509265362)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression1()\n",
    "classifier.fit(X_train, y_train,batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba170fb1",
   "metadata": {},
   "source": [
    "Using Mini-Batch training seems to have better performance for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58520bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression1(LogisticRegressionBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, X_train, y_train, batch_size=1, penalty = 'l1', lr=0.1):\n",
    "        # 0. Validate input\n",
    "        if (len(X_train) != len(y_train)): raise Exception(\"Match the data shape of X_train and y_train\")\n",
    "        self.X_size = len(X_train[0])\n",
    "        self.n_labels = len(y_train[0])\n",
    "        \n",
    "        \n",
    "        # 1. Assign random weights/biases\n",
    "        std=1e-4 # standard deviation\n",
    "        self.weights = std * np.random.randn(self.n_labels, self.X_size) # an array whose size is (n_features, n_classes)\n",
    "        self.biases = std * np.random.randn(self.n_labels) # an array of length n_classes\n",
    "        \n",
    "        # 2. Update \n",
    "        n_batches = int(np.ceil(len(y_train) / batch_size)); current_batch_size = batch_size\n",
    "        for batch_no in range(n_batches): # loop over the train batches\n",
    "            loss_batch = 0; \n",
    "            w_jacobian_batch = np.zeros(self.weights.shape);  b_jacobian_batch = np.zeros(self.biases.shape)\n",
    "            if (batch_no == n_batches): current_batch_size = len(y_train) % batch_size # size of the last batch\n",
    "            for trial in range(current_batch_size):\n",
    "                i = batch_no * current_batch_size + trial\n",
    "                loss, w_jacobian, b_jacobian = self.softmax_loss(X_train[i], y_train[i])\n",
    "                loss_batch -= loss; w_jacobian_batch += w_jacobian; b_jacobian_batch += b_jacobian\n",
    "            # average over the batch\n",
    "            loss_batch /= current_batch_size; w_jacobian_batch /= current_batch_size; b_jacobian_batch /= current_batch_size\n",
    "            self.gradient_descent(w_jacobian_batch, b_jacobian_batch, lr)\n",
    "            print(\"Epoch \" + str(batch_no+1) + \"\\a:\\a (loss: \" + str(loss_batch) + \")\")\n",
    "      \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP] *",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
