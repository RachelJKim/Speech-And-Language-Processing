{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98afd8c8",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a5b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951ea3d",
   "metadata": {},
   "source": [
    "# 0. Bigram Model (Chapter 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df22c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram:\n",
    "    def __init__(self):\n",
    "        # Pseudowords\n",
    "        self.pseudo_init = '<s>'; self.pseudo_termin = '</s>'\n",
    "        self.corpus_sequence = []\n",
    "        self.token_count = {self.pseudo_init:0, self.pseudo_termin:0}\n",
    "        self.bigram_count = {self.pseudo_init:{self.pseudo_init:0, self.pseudo_termin:0}, \n",
    "                             self.pseudo_termin:{self.pseudo_init:0, self.pseudo_termin:0}}\n",
    "\n",
    "    \"\"\" \n",
    "        Preprocessing\n",
    "            - preprocess any paragraph into a sequence of tokens\n",
    "    \"\"\"\n",
    "    def preprocess_corpus(self, paragraph):\n",
    "        corpus_sequence = []\n",
    "        for sentence in (paragraph.split('.')): # Divide by sentences\n",
    "            if (sentence == '') : corpus_sequence.append(self.pseudo_termin); break # Ignore the void sentence after the last period\n",
    "            corpus_sequence.append(self.pseudo_init) # Add a pseudo_init before every sentence.\n",
    "            for word in ((sentence.strip()).split(' ')): # Remove spaces and divide by words\n",
    "                corpus_sequence.append(word) # Add all word to the sequence\n",
    "            corpus_sequence.append(self.pseudo_termin) # Add a pseudo_termin after every sentence\n",
    "        return corpus_sequence[:-1] # In case the given paragraph doesn't end with a full sentence\n",
    "    \n",
    "    def preprocess_target(self, target):\n",
    "        target_sequence = []\n",
    "        for token in target.split(' '):\n",
    "            target_sequence.append(token)\n",
    "        return target_sequence\n",
    "                \n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "        Construct Bigram Count\n",
    "            - from the corpus sequence, construct bigram count table\n",
    "            - self.corpus_sequence should have been constructed in advance\n",
    "    \"\"\"\n",
    "    def count_bigram(self):\n",
    "        prev_token = self.pseudo_init; self.token_count[self.pseudo_init] += 1;\n",
    "        for token in self.corpus_sequence[1:]:\n",
    "            if token in self.token_count:\n",
    "                self.token_count[token] += 1\n",
    "            else: \n",
    "                self.token_count[token] = 1\n",
    "                # Add a new row(of token) to the bigram_count graph\n",
    "                self.bigram_count[token] = {token:0}\n",
    "                for existing_token in self.bigram_count.keys():\n",
    "                    self.bigram_count[token][existing_token] = 0\n",
    "                    # Add a new column to the bigram_count graph\n",
    "                    self.bigram_count[existing_token][token] = 0\n",
    "            self.bigram_count[prev_token][token] += 1\n",
    "            prev_token = token\n",
    "    \n",
    "    def display_bigram_count(self):\n",
    "        display(pd.DataFrame(self.bigram_count).transpose())\n",
    "    \n",
    "    def train(self, corpus, show_table=False):\n",
    "        self.corpus_sequence = self.preprocess_corpus(corpus)\n",
    "        self.count_bigram()\n",
    "        if (show_table == True):\n",
    "            self.display_bigram_count()\n",
    "    \n",
    "    def calculate_bigram(self, target):\n",
    "        # 1. Input Processing\n",
    "        target_sequence = self.preprocess_target(target)\n",
    "        \n",
    "        # 2. Bigram Count\n",
    "        #V = len(self.token_count.keys()) # Number of tokens (including pseudowords)\n",
    "        \n",
    "        n = len(target_sequence)\n",
    "        \n",
    "        p_total = 1\n",
    "        for k in range(1,n):\n",
    "            w_k = target_sequence[k]; w_kminus1 = target_sequence[k-1]\n",
    "            p_k_given_kminus1 = self.bigram_count[w_kminus1][w_k] / self.token_count[w_kminus1]\n",
    "            p_total *= p_k_given_kminus1\n",
    "        \n",
    "        return p_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a41563",
   "metadata": {},
   "source": [
    "## 0.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e747d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>Sam</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>and</th>\n",
       "      <th>ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       <s>  </s>  I  am  Sam  do  not  like  green  eggs  and  ham\n",
       "<s>      0     0  2   0    1   0    0     0      0     0    0    0\n",
       "</s>     2     0  0   0    0   0    0     0      0     0    0    0\n",
       "I        0     0  0   2    0   1    0     0      0     0    0    0\n",
       "am       0     1  0   0    1   0    0     0      0     0    0    0\n",
       "Sam      0     1  1   0    0   0    0     0      0     0    0    0\n",
       "do       0     0  0   0    0   0    1     0      0     0    0    0\n",
       "not      0     0  0   0    0   0    0     1      0     0    0    0\n",
       "like     0     0  0   0    0   0    0     0      1     0    0    0\n",
       "green    0     0  0   0    0   0    0     0      0     1    0    0\n",
       "eggs     0     0  0   0    0   0    0     0      0     0    1    0\n",
       "and      0     0  0   0    0   0    0     0      0     0    0    1\n",
       "ham      0     1  0   0    0   0    0     0      0     0    0    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "bigram_model = Bigram()\n",
    "bigram_model.train(corpus, show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef220a",
   "metadata": {},
   "source": [
    "## 0.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce7c838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(I|<s>) = 0.67\n",
      "P(Sam|<s>) = 0.33\n",
      "P(am|I) = 0.67\n",
      "P(</s>|Sam) = 0.5\n",
      "P(Sam|am) = 0.5\n",
      "P(do|I) = 0.33\n"
     ]
    }
   ],
   "source": [
    "targets = [\"<s> I\", \"<s> Sam\", \"I am\",\n",
    "           \"Sam </s>\", \"am Sam\", \"I do\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(\"P(\" + target.split()[-1] + \"|\" + target.split()[0] + \") = \" + str(round(bigram_model.calculate_bigram(target),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0d297",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b27da",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e669d8",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes Classifier (Chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "21978f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        import pandas as pd; import numpy as np; import math\n",
    "        self.classes = set(); self.features = set()\n",
    "        self.class_cnt = {}; self.feature_cnt = {}\n",
    "        self.data_cnt = None; self.X_shape = None;\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self._count(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # 0. Validate Input\n",
    "        if (X_test.shape[1:] != self.X_shape):\n",
    "            raise Exception(\"Match the data size of X_test\")\n",
    "        y_predict = []\n",
    "        for X in X_test:\n",
    "            y_predict.append(self._predict_single(X))\n",
    "        return y_predict\n",
    "    \n",
    "    def _predict_single(self, X):\n",
    "        probability = {}\n",
    "        for label in self.classes:\n",
    "            probability[label] = self._log_probability(X, label)\n",
    "        return max(probability, key=probability.get)\n",
    "    \n",
    "    \"\"\"\n",
    "        Count the number of..\n",
    "        \n",
    "        1. Classes throughout the whole dataset\n",
    "            - update self.class_cnt\n",
    "        \n",
    "        2. Feature occurence per class\n",
    "            - update self.feature_cnt\n",
    "    \"\"\"\n",
    "    def _count(self, X_train, y_train):\n",
    "        # 0. Validate Input\n",
    "        self.data_cnt = len(y_train);\n",
    "        if (len(X_train) != self.data_cnt): \n",
    "            raise Exception(\"Match the data size of X_train and y_train\")\n",
    "        else:\n",
    "            self.X_shape = X_train.shape[1:]\n",
    "        \n",
    "        for i in range(self.data_cnt):\n",
    "            X = X_train[i]; y = y_train[i]\n",
    "            \n",
    "            ### [Update Class Count] ###################################################\n",
    "            \n",
    "            # 1. Check if a new class appears\n",
    "            if (y not in self.classes):\n",
    "                # 1.1 Add the class to class set and self.class_cnt\n",
    "                self.classes.add(y) \n",
    "                self.class_cnt[y] = 0\n",
    "                # 1.2 Create a column for the class in count table\n",
    "                self.feature_cnt[y] = dict(zip(list(self.features), np.zeros(len(self.features), dtype=int))) \n",
    "            \n",
    "            # 2. Update the class count\n",
    "            self.class_cnt[y] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "            \n",
    "            ### [Update Feature Count] #################################################\n",
    "            \n",
    "            for x_i in X:\n",
    "                # 1. Check if a new feature appears\n",
    "                if (x_i not in self.features):\n",
    "                    # 1.1 Add the feature to the feature set\n",
    "                    self.features.add(x_i)\n",
    "                    # 1.2 Add a row for the feature in count table\n",
    "                    for _class in self.feature_cnt.keys():\n",
    "                        self.feature_cnt[_class][x_i] = 0\n",
    "                        \n",
    "                # 2. Update the feature count\n",
    "                self.feature_cnt[y][x_i] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute P(X|y)\n",
    "        \n",
    "        1. term1: P(y)\n",
    "        2. term2: P(X|y) -> with Laplace Smoothing\n",
    "            \n",
    "    \"\"\"\n",
    "    def _log_probability(self, X, y):\n",
    "        term1 = math.log(self.class_cnt[y] / self.data_cnt, 10)\n",
    "        \n",
    "        term2 = 0\n",
    "        features_sum = sum(self.feature_cnt[y].values()) + len(self.features)\n",
    "        for x_i in X:\n",
    "            try:\n",
    "                term2 += math.log(self.feature_cnt[y][x_i] + 1, 10)\n",
    "            except:\n",
    "                term2 = 0\n",
    "        \n",
    "        return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "743e7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[\"A\", \"B\", \"C\"], [\"A\", \"B\", \"B\"], [\"D\", \"F\", \"A\"], [\"C\", \"B\", \"B\"]])\n",
    "y_train = np.array([\"Pass\", \"Pass\", \"Fail\", \"Fail\"])\n",
    "\n",
    "X_test1 = np.array([[\"A\", \"C\", \"B\"],\n",
    "                    [\"A\", \"C\", \"X\"],  # Unseen data -> smoothed by Laplace\n",
    "                    [\"F\", \"D\", \"B\"],\n",
    "                    [\"X\", \"Y\", \"Z\"]]) # Unseen data -> smoothed by Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8b10d6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pass', 'Fail', 'Fail', 'Fail']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = NaiveBayes()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "classifier.predict(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f26c9f",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression Classifier (Chapter 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b128bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.classes = set()\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP] *",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
