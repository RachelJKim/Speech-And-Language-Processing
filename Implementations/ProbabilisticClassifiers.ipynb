{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e75c68c",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d7d279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3d368",
   "metadata": {},
   "source": [
    "# 0. Bigram Model (Chapter 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c985557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram:\n",
    "    def __init__(self):\n",
    "        # Pseudowords\n",
    "        self.pseudo_init = '<s>'; self.pseudo_termin = '</s>'\n",
    "        self.corpus_sequence = []\n",
    "        self.token_count = {self.pseudo_init:0, self.pseudo_termin:0}\n",
    "        self.bigram_count = {self.pseudo_init:{self.pseudo_init:0, self.pseudo_termin:0}, \n",
    "                             self.pseudo_termin:{self.pseudo_init:0, self.pseudo_termin:0}}\n",
    "\n",
    "    \"\"\" \n",
    "        Preprocessing\n",
    "            - preprocess any paragraph into a sequence of tokens\n",
    "    \"\"\"\n",
    "    def preprocess_corpus(self, paragraph):\n",
    "        corpus_sequence = []\n",
    "        for sentence in (paragraph.split('.')): # Divide by sentences\n",
    "            if (sentence == '') : corpus_sequence.append(self.pseudo_termin); break # Ignore the void sentence after the last period\n",
    "            corpus_sequence.append(self.pseudo_init) # Add a pseudo_init before every sentence.\n",
    "            for word in ((sentence.strip()).split(' ')): # Remove spaces and divide by words\n",
    "                corpus_sequence.append(word) # Add all word to the sequence\n",
    "            corpus_sequence.append(self.pseudo_termin) # Add a pseudo_termin after every sentence\n",
    "        return corpus_sequence[:-1] # In case the given paragraph doesn't end with a full sentence\n",
    "    \n",
    "    def preprocess_target(self, target):\n",
    "        target_sequence = []\n",
    "        for token in target.split(' '):\n",
    "            target_sequence.append(token)\n",
    "        return target_sequence\n",
    "                \n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "        Construct Bigram Count\n",
    "            - from the corpus sequence, construct bigram count table\n",
    "            - self.corpus_sequence should have been constructed in advance\n",
    "    \"\"\"\n",
    "    def count_bigram(self):\n",
    "        prev_token = self.pseudo_init; self.token_count[self.pseudo_init] += 1;\n",
    "        for token in self.corpus_sequence[1:]:\n",
    "            if token in self.token_count:\n",
    "                self.token_count[token] += 1\n",
    "            else: \n",
    "                self.token_count[token] = 1\n",
    "                # Add a new row(of token) to the bigram_count graph\n",
    "                self.bigram_count[token] = {token:0}\n",
    "                for existing_token in self.bigram_count.keys():\n",
    "                    self.bigram_count[token][existing_token] = 0\n",
    "                    # Add a new column to the bigram_count graph\n",
    "                    self.bigram_count[existing_token][token] = 0\n",
    "            self.bigram_count[prev_token][token] += 1\n",
    "            prev_token = token\n",
    "    \n",
    "    def display_bigram_count(self):\n",
    "        display(pd.DataFrame(self.bigram_count).transpose())\n",
    "    \n",
    "    def train(self, corpus, show_table=False):\n",
    "        self.corpus_sequence = self.preprocess_corpus(corpus)\n",
    "        self.count_bigram()\n",
    "        if (show_table == True):\n",
    "            self.display_bigram_count()\n",
    "    \n",
    "    def calculate_bigram(self, target):\n",
    "        # 1. Input Processing\n",
    "        target_sequence = self.preprocess_target(target)\n",
    "        \n",
    "        # 2. Bigram Count\n",
    "        #V = len(self.token_count.keys()) # Number of tokens (including pseudowords)\n",
    "        \n",
    "        n = len(target_sequence)\n",
    "        \n",
    "        p_total = 1\n",
    "        for k in range(1,n):\n",
    "            w_k = target_sequence[k]; w_kminus1 = target_sequence[k-1]\n",
    "            p_k_given_kminus1 = self.bigram_count[w_kminus1][w_k] / self.token_count[w_kminus1]\n",
    "            p_total *= p_k_given_kminus1\n",
    "        \n",
    "        return p_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657c425",
   "metadata": {},
   "source": [
    "## 0.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3324be46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>Sam</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>and</th>\n",
       "      <th>ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       <s>  </s>  I  am  Sam  do  not  like  green  eggs  and  ham\n",
       "<s>      0     0  2   0    1   0    0     0      0     0    0    0\n",
       "</s>     2     0  0   0    0   0    0     0      0     0    0    0\n",
       "I        0     0  0   2    0   1    0     0      0     0    0    0\n",
       "am       0     1  0   0    1   0    0     0      0     0    0    0\n",
       "Sam      0     1  1   0    0   0    0     0      0     0    0    0\n",
       "do       0     0  0   0    0   0    1     0      0     0    0    0\n",
       "not      0     0  0   0    0   0    0     1      0     0    0    0\n",
       "like     0     0  0   0    0   0    0     0      1     0    0    0\n",
       "green    0     0  0   0    0   0    0     0      0     1    0    0\n",
       "eggs     0     0  0   0    0   0    0     0      0     0    1    0\n",
       "and      0     0  0   0    0   0    0     0      0     0    0    1\n",
       "ham      0     1  0   0    0   0    0     0      0     0    0    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "bigram_model = Bigram()\n",
    "bigram_model.train(corpus, show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2d51f",
   "metadata": {},
   "source": [
    "## 0.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f7fb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(I|<s>) = 0.67\n",
      "P(Sam|<s>) = 0.33\n",
      "P(am|I) = 0.67\n",
      "P(</s>|Sam) = 0.5\n",
      "P(Sam|am) = 0.5\n",
      "P(do|I) = 0.33\n"
     ]
    }
   ],
   "source": [
    "targets = [\"<s> I\", \"<s> Sam\", \"I am\",\n",
    "           \"Sam </s>\", \"am Sam\", \"I do\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(\"P(\" + target.split()[-1] + \"|\" + target.split()[0] + \") = \" + str(round(bigram_model.calculate_bigram(target),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1de41",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdeed8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec0f03",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes Classifier (Chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7111865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        import pandas as pd; import numpy as np; import math\n",
    "        self.classes = set(); self.features = set()\n",
    "        self.class_cnt = {}; self.feature_cnt = {}\n",
    "        self.data_cnt = None; self.X_shape = None;\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self._count(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # 0. Validate Input\n",
    "        if (X_test.shape[1:] != self.X_shape):\n",
    "            raise Exception(\"Match the data size of X_test\")\n",
    "        y_predict = []\n",
    "        for X in X_test:\n",
    "            y_predict.append(self._predict_single(X))\n",
    "        return y_predict\n",
    "    \n",
    "    def _predict_single(self, X):\n",
    "        probability = {}\n",
    "        for label in self.classes:\n",
    "            probability[label] = self._log_probability(X, label)\n",
    "        return max(probability, key=probability.get)\n",
    "    \n",
    "    \"\"\"\n",
    "        Count the number of..\n",
    "        \n",
    "        1. Classes throughout the whole dataset\n",
    "            - update self.class_cnt\n",
    "        \n",
    "        2. Feature occurence per class\n",
    "            - update self.feature_cnt\n",
    "    \"\"\"\n",
    "    def _count(self, X_train, y_train):\n",
    "        # 0. Validate Input\n",
    "        self.data_cnt = len(y_train);\n",
    "        if (len(X_train) != self.data_cnt): \n",
    "            raise Exception(\"Match the data size of X_train and y_train\")\n",
    "        else:\n",
    "            self.X_shape = X_train.shape[1:]\n",
    "        \n",
    "        for i in range(self.data_cnt):\n",
    "            X = X_train[i]; y = y_train[i]\n",
    "            \n",
    "            ### [Update Class Count] ###################################################\n",
    "            \n",
    "            # 1. Check if a new class appears\n",
    "            if (y not in self.classes):\n",
    "                # 1.1 Add the class to class set and self.class_cnt\n",
    "                self.classes.add(y) \n",
    "                self.class_cnt[y] = 0\n",
    "                # 1.2 Create a column for the class in count table\n",
    "                self.feature_cnt[y] = dict(zip(list(self.features), np.zeros(len(self.features), dtype=int))) \n",
    "            \n",
    "            # 2. Update the class count\n",
    "            self.class_cnt[y] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "            \n",
    "            ### [Update Feature Count] #################################################\n",
    "            \n",
    "            for x_i in X:\n",
    "                # 1. Check if a new feature appears\n",
    "                if (x_i not in self.features):\n",
    "                    # 1.1 Add the feature to the feature set\n",
    "                    self.features.add(x_i)\n",
    "                    # 1.2 Add a row for the feature in count table\n",
    "                    for _class in self.feature_cnt.keys():\n",
    "                        self.feature_cnt[_class][x_i] = 0\n",
    "                        \n",
    "                # 2. Update the feature count\n",
    "                self.feature_cnt[y][x_i] += 1\n",
    "            \n",
    "            ############################################################################\n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute P(X|y)\n",
    "        \n",
    "        1. term1: P(y)\n",
    "        2. term2: P(X|y) -> with Laplace Smoothing\n",
    "            \n",
    "    \"\"\"\n",
    "    def _log_probability(self, X, y):\n",
    "        term1 = math.log(self.class_cnt[y] / self.data_cnt, 10)\n",
    "        \n",
    "        term2 = 0\n",
    "        features_sum = sum(self.feature_cnt[y].values()) + len(self.features)\n",
    "        for x_i in X:\n",
    "            try:\n",
    "                term2 += math.log(self.feature_cnt[y][x_i] + 1, 10)\n",
    "            except:\n",
    "                term2 = 0\n",
    "        \n",
    "        return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6d357c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[\"A\", \"B\", \"C\"], [\"A\", \"B\", \"B\"], [\"D\", \"F\", \"A\"], [\"C\", \"B\", \"B\"]])\n",
    "y_train = np.array([\"Pass\", \"Pass\", \"Fail\", \"Fail\"])\n",
    "\n",
    "X_test1 = np.array([[\"A\", \"C\", \"B\"],\n",
    "                    [\"A\", \"C\", \"X\"],  # Unseen data -> smoothed by Laplace\n",
    "                    [\"F\", \"D\", \"B\"],\n",
    "                    [\"X\", \"Y\", \"Z\"]]) # Unseen data -> smoothed by Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "391a9156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pass', 'Fail', 'Fail', 'Fail']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = NaiveBayes()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "classifier.predict(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848a683",
   "metadata": {},
   "source": [
    "# 2. Softmax Regression Classifier (Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55c77f",
   "metadata": {},
   "source": [
    "## 2.1 Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dace751",
   "metadata": {},
   "source": [
    "* Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "7a8d69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBase(object):\n",
    "    def __init__(self):\n",
    "        import numpy as np;\n",
    "        self.n_labels = None\n",
    "        self.X_size = None\n",
    "        self.weights = None; self.biases = None\n",
    "    \n",
    "    \"\"\"\n",
    "        Training Data\n",
    "            - y_train: An array of one-hot encoded vectors\n",
    "            - lr: learning rate\n",
    "    \"\"\"\n",
    "    def fit(self, X_train, y_train, lr=0.1):\n",
    "        # 0. Validate input\n",
    "        if (len(X_train) != len(y_train)): raise Exception(\"Match the data shape of X_train and y_train\")\n",
    "        self.X_size = len(X_train[0])\n",
    "        self.n_labels = len(y_train[0])\n",
    "        \n",
    "        # 1. Assign random weights/biases\n",
    "        std=1e-4 # standard deviation\n",
    "        self.weights = std * np.random.randn(self.n_labels, self.X_size) # an array whose size is (n_features, n_classes)\n",
    "        self.biases = std * np.random.randn(self.n_labels) # an array of length n_classes\n",
    "        \n",
    "        # 2. Update\n",
    "        for i in range(len(y_train)): # loop over the train data\n",
    "            loss, w_jacobian, b_jacobian = self.softmax_loss(X_train[i], y_train[i])\n",
    "            self.gradient_descent(w_jacobian, b_jacobian, lr)\n",
    "            print(\"Epoch \" + str(i+1) + \"\\a:\\a (loss: \" + str(loss) + \")\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # 0. Validate input\n",
    "        if (X.shape[0] != self.X_size): raise Exception(\"Match data shape of X_test: \" + str(self.X_size))\n",
    "        \n",
    "        # 1. Predict Label(class)\n",
    "        prob = np.dot(self.weights, X) + self.biases\n",
    "        label = np.argmax(prob)\n",
    "        \n",
    "        # 2. Convert the label into a one-hot encoded vector\n",
    "        label_vector = np.zeros(self.n_labels, dtype=int); label_vector[label] = 1\n",
    "        return label_vector\n",
    "        \n",
    "        \n",
    "        \n",
    "    def softmax_loss(self, X, y):\n",
    "        \"\"\"\n",
    "            Compute the Loss and Gradients for a binary classifier\n",
    "            Return:\n",
    "                - Loss\n",
    "                - Gradient\n",
    "        \"\"\"\n",
    "        # 1. Compute the summ for softmax\n",
    "        summ = 0\n",
    "        for k in range(self.n_labels): # loop through the classes\n",
    "            w_k = self.weights[k]; b_k = self.biases[k]\n",
    "            summ += np.exp(np.dot(w_k, X) + b_k)\n",
    "        \n",
    "        # 2. Update the gradient matrixes, and calculate the loss\n",
    "        loss = 0; \n",
    "        w_jacobian = np.zeros(self.weights.shape); b_jacobian = np.zeros(self.biases.shape)\n",
    "        for k in range(self.n_labels):\n",
    "            w_k = self.weights[k]; b_k = self.biases[k]\n",
    "            softmax = (np.dot(w_k, X) + b_k) / summ # softmax\n",
    "            gradient = softmax - y[k]\n",
    "            for i in range(self.X_size):\n",
    "                w_jacobian[k][i] = gradient * X[i]\n",
    "            b_jacobian[k] = gradient\n",
    "            \n",
    "            # Loss -= softmax for the correct class\n",
    "            if (y[k] == 1): loss -= softmax;\n",
    "        \n",
    "        return loss, w_jacobian, b_jacobian\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, w_jacobian, b_jacobian, lr):\n",
    "        \"\"\"\n",
    "            Update the weights and biases\n",
    "        \"\"\"\n",
    "        self.weights -= lr * w_jacobian\n",
    "        self.biases -= lr * b_jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae52514",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3bd21",
   "metadata": {},
   "source": [
    "### 1) Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "49a6c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = []; y_train = []\n",
    "\n",
    "# Create random data points\n",
    "data_per_label = 50\n",
    "labels = [[0,0,1], [0,1,0], [1,0,0]]; centroids = [(2,2), (3,5), (5,2)]\n",
    "for i in range(len(labels)):\n",
    "    xs = centroids[i][0] + (np.random.random_sample((data_per_label,)) - 0.5) * 2\n",
    "    ys = centroids[i][1] + (np.random.random_sample((data_per_label,)) - 0.5) * 2\n",
    "    X_train += list(zip(xs, ys))\n",
    "    y_train += [labels[i] for j in range(data_per_label)]\n",
    "    \n",
    "# Shuffle\n",
    "shuffle = list(zip(X_train, y_train)); np.random.shuffle(shuffle)\n",
    "X_train = list(map(lambda pair: pair[0], shuffle))\n",
    "y_train = list(map(lambda pair: pair[1], shuffle))\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.scatter([point[0] for point in X_train], [point[1] for point in X_train], c=y_train, label=y_train)\n",
    "plt.xlim([0, 7]); plt.ylim([0, 7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4fabb",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "71bab2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\u0007:\u0007 (loss: 7.567970068017323e-05)\n",
      "Epoch 2\u0007:\u0007 (loss: 4.458578051525835e-05)\n",
      "Epoch 3\u0007:\u0007 (loss: -0.14718638202086404)\n",
      "Epoch 4\u0007:\u0007 (loss: -0.03470846060025884)\n",
      "Epoch 5\u0007:\u0007 (loss: 3.6284800850989726e-06)\n",
      "Epoch 6\u0007:\u0007 (loss: -0.007580072328098434)\n",
      "Epoch 7\u0007:\u0007 (loss: -0.012281755638639731)\n",
      "Epoch 8\u0007:\u0007 (loss: -0.0005650566002391251)\n",
      "Epoch 9\u0007:\u0007 (loss: -0.0005562987996697162)\n",
      "Epoch 10\u0007:\u0007 (loss: -1.3945581786327189e-05)\n",
      "Epoch 11\u0007:\u0007 (loss: -0.00011875707391649622)\n",
      "Epoch 12\u0007:\u0007 (loss: -0.00016533524704945952)\n",
      "Epoch 13\u0007:\u0007 (loss: -3.287168564847606e-07)\n",
      "Epoch 14\u0007:\u0007 (loss: -0.001433481186483145)\n",
      "Epoch 15\u0007:\u0007 (loss: -9.421816876838781e-07)\n",
      "Epoch 16\u0007:\u0007 (loss: -2.48262690699979e-09)\n",
      "Epoch 17\u0007:\u0007 (loss: -4.398479264583057e-11)\n",
      "Epoch 18\u0007:\u0007 (loss: -2.951284451414821e-12)\n",
      "Epoch 19\u0007:\u0007 (loss: -2.9075448438540274e-07)\n",
      "Epoch 20\u0007:\u0007 (loss: -8.536468198204844e-13)\n",
      "Epoch 21\u0007:\u0007 (loss: -3.772609632265766e-08)\n",
      "Epoch 22\u0007:\u0007 (loss: -2.0962154553936755e-12)\n",
      "Epoch 23\u0007:\u0007 (loss: -6.65782178856883e-08)\n",
      "Epoch 24\u0007:\u0007 (loss: -7.194742788206239e-07)\n",
      "Epoch 25\u0007:\u0007 (loss: -1.1533654116656129e-08)\n",
      "Epoch 26\u0007:\u0007 (loss: -3.612289483292162e-15)\n",
      "Epoch 27\u0007:\u0007 (loss: -2.786683775211246e-14)\n",
      "Epoch 28\u0007:\u0007 (loss: -1.0606868306337757e-12)\n",
      "Epoch 29\u0007:\u0007 (loss: -1.434344074376995e-14)\n",
      "Epoch 30\u0007:\u0007 (loss: -3.674671229896576e-13)\n",
      "Epoch 31\u0007:\u0007 (loss: -2.4194502424728018e-08)\n",
      "Epoch 32\u0007:\u0007 (loss: -2.992700671570207e-07)\n",
      "Epoch 33\u0007:\u0007 (loss: -9.269854536321048e-14)\n",
      "Epoch 34\u0007:\u0007 (loss: -4.8124659695114475e-14)\n",
      "Epoch 35\u0007:\u0007 (loss: -1.0912415835756724e-12)\n",
      "Epoch 36\u0007:\u0007 (loss: -2.7244843973619516e-17)\n",
      "Epoch 37\u0007:\u0007 (loss: -2.851965123381392e-06)\n",
      "Epoch 38\u0007:\u0007 (loss: -2.4074230310935997e-16)\n",
      "Epoch 39\u0007:\u0007 (loss: -1.231318680954655e-15)\n",
      "Epoch 40\u0007:\u0007 (loss: -7.216659157655586e-15)\n",
      "Epoch 41\u0007:\u0007 (loss: -2.203609377020482e-21)\n",
      "Epoch 42\u0007:\u0007 (loss: -2.3643148990375696e-17)\n",
      "Epoch 43\u0007:\u0007 (loss: -8.611210513652113e-21)\n",
      "Epoch 44\u0007:\u0007 (loss: -2.0364276720387399e-19)\n",
      "Epoch 45\u0007:\u0007 (loss: -6.4510284149220645e-25)\n",
      "Epoch 46\u0007:\u0007 (loss: -3.02914400502307e-17)\n",
      "Epoch 47\u0007:\u0007 (loss: -1.0464648178213891e-22)\n",
      "Epoch 48\u0007:\u0007 (loss: -9.304738432112068e-10)\n",
      "Epoch 49\u0007:\u0007 (loss: -1.3119997070205103e-19)\n",
      "Epoch 50\u0007:\u0007 (loss: -7.186852570244841e-13)\n",
      "Epoch 51\u0007:\u0007 (loss: -7.414905338757917e-24)\n",
      "Epoch 52\u0007:\u0007 (loss: -4.4445481327576034e-26)\n",
      "Epoch 53\u0007:\u0007 (loss: -3.3675559254746726e-08)\n",
      "Epoch 54\u0007:\u0007 (loss: -3.383383398610319e-15)\n",
      "Epoch 55\u0007:\u0007 (loss: -2.3791514722795566e-14)\n",
      "Epoch 56\u0007:\u0007 (loss: -2.939550988760114e-25)\n",
      "Epoch 57\u0007:\u0007 (loss: -1.1317243824597735e-25)\n",
      "Epoch 58\u0007:\u0007 (loss: -6.109367015773325e-10)\n",
      "Epoch 59\u0007:\u0007 (loss: -1.2249064028672186e-22)\n",
      "Epoch 60\u0007:\u0007 (loss: -1.1499810904882735e-32)\n",
      "Epoch 61\u0007:\u0007 (loss: -7.591325417723735e-28)\n",
      "Epoch 62\u0007:\u0007 (loss: -2.834970105321701e-13)\n",
      "Epoch 63\u0007:\u0007 (loss: -3.299875386824288e-15)\n",
      "Epoch 64\u0007:\u0007 (loss: -3.0966124284424588e-27)\n",
      "Epoch 65\u0007:\u0007 (loss: -2.9328935803088792e-33)\n",
      "Epoch 66\u0007:\u0007 (loss: -9.412667977044505e-15)\n",
      "Epoch 67\u0007:\u0007 (loss: -2.4198370260222387e-30)\n",
      "Epoch 68\u0007:\u0007 (loss: -2.6557263270542344e-23)\n",
      "Epoch 69\u0007:\u0007 (loss: -2.039096791022567e-29)\n",
      "Epoch 70\u0007:\u0007 (loss: -1.1281849394203328e-15)\n",
      "Epoch 71\u0007:\u0007 (loss: -2.4574129405216178e-36)\n",
      "Epoch 72\u0007:\u0007 (loss: -1.461692062994502e-18)\n",
      "Epoch 73\u0007:\u0007 (loss: -1.251549412257687e-12)\n",
      "Epoch 74\u0007:\u0007 (loss: -3.4383564566487697e-40)\n",
      "Epoch 75\u0007:\u0007 (loss: -4.745300176976919e-27)\n",
      "Epoch 76\u0007:\u0007 (loss: -3.027234014505698e-41)\n",
      "Epoch 77\u0007:\u0007 (loss: -3.1952845692924516e-37)\n",
      "Epoch 78\u0007:\u0007 (loss: -2.8434366200980755e-31)\n",
      "Epoch 79\u0007:\u0007 (loss: -5.805717111254674e-19)\n",
      "Epoch 80\u0007:\u0007 (loss: -1.1158185929746613e-35)\n",
      "Epoch 81\u0007:\u0007 (loss: -1.6302759706195967e-33)\n",
      "Epoch 82\u0007:\u0007 (loss: -1.493178743065517e-13)\n",
      "Epoch 83\u0007:\u0007 (loss: -6.174545719419512e-38)\n",
      "Epoch 84\u0007:\u0007 (loss: -7.891514276866594e-42)\n",
      "Epoch 85\u0007:\u0007 (loss: -3.0059974231009845e-42)\n",
      "Epoch 86\u0007:\u0007 (loss: -5.1708307053550395e-23)\n",
      "Epoch 87\u0007:\u0007 (loss: -5.735539588121175e-39)\n",
      "Epoch 88\u0007:\u0007 (loss: -1.7803764674306855e-18)\n",
      "Epoch 89\u0007:\u0007 (loss: -1.291513096951921e-22)\n",
      "Epoch 90\u0007:\u0007 (loss: -3.9945419087052825e-44)\n",
      "Epoch 91\u0007:\u0007 (loss: -2.542388981590382e-25)\n",
      "Epoch 92\u0007:\u0007 (loss: -1.9699291614226975e-41)\n",
      "Epoch 93\u0007:\u0007 (loss: -1.5048024069268785e-46)\n",
      "Epoch 94\u0007:\u0007 (loss: -1.0753176764173771e-30)\n",
      "Epoch 95\u0007:\u0007 (loss: -1.1121008431867555e-43)\n",
      "Epoch 96\u0007:\u0007 (loss: -1.3352327888762263e-14)\n",
      "Epoch 97\u0007:\u0007 (loss: -1.0745535221062732e-26)\n",
      "Epoch 98\u0007:\u0007 (loss: -3.6877694622629865e-24)\n",
      "Epoch 99\u0007:\u0007 (loss: -1.9434089017965503e-49)\n",
      "Epoch 100\u0007:\u0007 (loss: -2.19764125812641e-23)\n",
      "Epoch 101\u0007:\u0007 (loss: -2.4014969171101585e-43)\n",
      "Epoch 102\u0007:\u0007 (loss: -2.2575992788539773e-42)\n",
      "Epoch 103\u0007:\u0007 (loss: -6.107940842699158e-49)\n",
      "Epoch 104\u0007:\u0007 (loss: -3.2281312187428447e-49)\n",
      "Epoch 105\u0007:\u0007 (loss: -2.8646000525757264e-31)\n",
      "Epoch 106\u0007:\u0007 (loss: -2.134017188327445e-44)\n",
      "Epoch 107\u0007:\u0007 (loss: -1.1486713783279643e-43)\n",
      "Epoch 108\u0007:\u0007 (loss: -4.114270733092095e-60)\n",
      "Epoch 109\u0007:\u0007 (loss: -1.1623628356849932e-22)\n",
      "Epoch 110\u0007:\u0007 (loss: -1.2154358725722958e-54)\n",
      "Epoch 111\u0007:\u0007 (loss: -1.0354113268746769e-23)\n",
      "Epoch 112\u0007:\u0007 (loss: -1.310635021667669e-39)\n",
      "Epoch 113\u0007:\u0007 (loss: -4.165762479471668e-56)\n",
      "Epoch 114\u0007:\u0007 (loss: -7.615316021629243e-68)\n",
      "Epoch 115\u0007:\u0007 (loss: -9.946564004285405e-41)\n",
      "Epoch 116\u0007:\u0007 (loss: -1.2473542843107452e-49)\n",
      "Epoch 117\u0007:\u0007 (loss: -8.414158453209726e-42)\n",
      "Epoch 118\u0007:\u0007 (loss: -6.645488455834508e-64)\n",
      "Epoch 119\u0007:\u0007 (loss: -1.0393050348094572e-67)\n",
      "Epoch 120\u0007:\u0007 (loss: -3.260515552233743e-59)\n",
      "Epoch 121\u0007:\u0007 (loss: -2.7030441610840368e-36)\n",
      "Epoch 122\u0007:\u0007 (loss: -1.8617954731626096e-54)\n",
      "Epoch 123\u0007:\u0007 (loss: -9.2268559086219e-21)\n",
      "Epoch 124\u0007:\u0007 (loss: -1.788371919226123e-35)\n",
      "Epoch 125\u0007:\u0007 (loss: -3.2932458680618844e-66)\n",
      "Epoch 126\u0007:\u0007 (loss: -3.4361216607155425e-53)\n",
      "Epoch 127\u0007:\u0007 (loss: -1.62409704593356e-55)\n",
      "Epoch 128\u0007:\u0007 (loss: -1.2351640279668994e-31)\n",
      "Epoch 129\u0007:\u0007 (loss: -3.1726142734455526e-57)\n",
      "Epoch 130\u0007:\u0007 (loss: -3.297945775354151e-55)\n",
      "Epoch 131\u0007:\u0007 (loss: -1.3456922014417325e-40)\n",
      "Epoch 132\u0007:\u0007 (loss: -4.128442557332121e-60)\n",
      "Epoch 133\u0007:\u0007 (loss: -4.981211765119906e-71)\n",
      "Epoch 134\u0007:\u0007 (loss: -2.5952834519633883e-68)\n",
      "Epoch 135\u0007:\u0007 (loss: -5.004287163631144e-38)\n",
      "Epoch 136\u0007:\u0007 (loss: -3.8996841960119496e-55)\n",
      "Epoch 137\u0007:\u0007 (loss: -7.003788197841041e-69)\n",
      "Epoch 138\u0007:\u0007 (loss: -2.8009436371650208e-64)\n",
      "Epoch 139\u0007:\u0007 (loss: -1.0719008814203385e-36)\n",
      "Epoch 140\u0007:\u0007 (loss: -1.2947316924031315e-58)\n",
      "Epoch 141\u0007:\u0007 (loss: -9.421858677097028e-53)\n",
      "Epoch 142\u0007:\u0007 (loss: -3.9493382470076546e-64)\n",
      "Epoch 143\u0007:\u0007 (loss: -9.932111213289572e-79)\n",
      "Epoch 144\u0007:\u0007 (loss: -2.284667000513573e-42)\n",
      "Epoch 145\u0007:\u0007 (loss: -2.475733544386434e-39)\n",
      "Epoch 146\u0007:\u0007 (loss: -8.798404058444581e-40)\n",
      "Epoch 147\u0007:\u0007 (loss: -6.568222368237116e-65)\n",
      "Epoch 148\u0007:\u0007 (loss: -9.61535217490634e-59)\n",
      "Epoch 149\u0007:\u0007 (loss: -4.279698146026759e-86)\n",
      "Epoch 150\u0007:\u0007 (loss: -1.2810687524723347e-49)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionBase()\n",
    "classifier.fit(X_train, y_train, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe6c65",
   "metadata": {},
   "source": [
    "Here, we can see that the training is not done well.\n",
    "The loss doesn't converge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b76a94",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9f2b4",
   "metadata": {},
   "source": [
    "## 2.2 (Update) Mini-Batch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e36125",
   "metadata": {},
   "source": [
    "- Mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "5543db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression1(LogisticRegressionBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, X_train, y_train, batch_size=1, lr=0.1):\n",
    "        # 0. Validate input\n",
    "        if (len(X_train) != len(y_train)): raise Exception(\"Match the data shape of X_train and y_train\")\n",
    "        self.X_size = len(X_train[0])\n",
    "        self.n_labels = len(y_train[0])\n",
    "        \n",
    "        \n",
    "        # 1. Assign random weights/biases\n",
    "        std=1e-4 # standard deviation\n",
    "        self.weights = std * np.random.randn(self.n_labels, self.X_size) # an array whose size is (n_features, n_classes)\n",
    "        self.biases = std * np.random.randn(self.n_labels) # an array of length n_classes\n",
    "        \n",
    "        # 2. Update \n",
    "        n_batches = int(np.ceil(len(y_train) / batch_size)); current_batch_size = batch_size\n",
    "        for batch_no in range(n_batches): # loop over the train batches\n",
    "            loss_batch = 0; \n",
    "            w_jacobian_batch = np.zeros(self.weights.shape);  b_jacobian_batch = np.zeros(self.biases.shape)\n",
    "            if (batch_no == n_batches-1): current_batch_size = len(y_train) % batch_size # size of the last batch\n",
    "            if current_batch_size == 0: continue\n",
    "            for trial in range(current_batch_size):\n",
    "                i = batch_no * current_batch_size + trial\n",
    "                loss, w_jacobian, b_jacobian = self.softmax_loss(X_train[i], y_train[i])\n",
    "                loss_batch -= loss; w_jacobian_batch += w_jacobian; b_jacobian_batch += b_jacobian\n",
    "            # average over the batch\n",
    "            loss_batch /= current_batch_size; w_jacobian_batch /= current_batch_size; b_jacobian_batch /= current_batch_size\n",
    "            self.gradient_descent(w_jacobian_batch, b_jacobian_batch, lr)\n",
    "            print(\"Epoch \" + str(batch_no+1) + \"\\a:\\a (loss: \" + str(loss_batch) + \")\")\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "10b11da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\u0007:\u0007 (loss: 0.0001041312499095087)\n",
      "Epoch 2\u0007:\u0007 (loss: 0.1350156024685745)\n",
      "Epoch 3\u0007:\u0007 (loss: 0.05088636061812113)\n",
      "Epoch 4\u0007:\u0007 (loss: 0.09986975216290897)\n",
      "Epoch 5\u0007:\u0007 (loss: 0.060785792765779736)\n",
      "Epoch 6\u0007:\u0007 (loss: 0.04577829836880013)\n",
      "Epoch 7\u0007:\u0007 (loss: 0.030693371991226646)\n",
      "Epoch 8\u0007:\u0007 (loss: 0.016989667667135163)\n",
      "Epoch 9\u0007:\u0007 (loss: 0.013665965374651364)\n",
      "Epoch 10\u0007:\u0007 (loss: 0.010861600935802304)\n",
      "Epoch 11\u0007:\u0007 (loss: 0.003083864686731309)\n",
      "Epoch 12\u0007:\u0007 (loss: 0.001446249686945953)\n",
      "Epoch 13\u0007:\u0007 (loss: 0.002029162164301916)\n",
      "Epoch 14\u0007:\u0007 (loss: 0.0003365427430256697)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression1()\n",
    "classifier.fit(X_train, y_train,batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b091a1e",
   "metadata": {},
   "source": [
    "Using Mini-Batch training seems to have better performance for this data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58042c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806d1b5",
   "metadata": {},
   "source": [
    "## 2.3 (Update) Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "279762f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression2(LogisticRegressionBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def fit(self, X_train, y_train, batch_size=1, lr=0.1, penalty = None, alpha=0.1):\n",
    "        # 0. Validate input\n",
    "        if (len(X_train) != len(y_train)): raise Exception(\"Match the data shape of X_train and y_train\")\n",
    "        self.X_size = len(X_train[0])\n",
    "        self.n_labels = len(y_train[0])\n",
    "        \n",
    "        \n",
    "        # 1. Assign random weights/biases\n",
    "        std=1e-4 # standard deviation\n",
    "        self.weights = std * np.random.randn(self.n_labels, self.X_size) # an array whose size is (n_features, n_classes)\n",
    "        self.biases = std * np.random.randn(self.n_labels) # an array of length n_classes\n",
    "        \n",
    "        # 2. Update \n",
    "        n_batches = int(np.ceil(len(y_train) / batch_size)); current_batch_size = batch_size\n",
    "        for batch_no in range(n_batches): # loop over the train batches\n",
    "            loss_batch = 0; \n",
    "            w_jacobian_batch = np.zeros(self.weights.shape);  b_jacobian_batch = np.zeros(self.biases.shape)\n",
    "            if (batch_no == n_batches-1): current_batch_size = len(y_train) % batch_size # size of the last batch\n",
    "            if current_batch_size == 0: continue\n",
    "            for trial in range(current_batch_size):\n",
    "                i = batch_no * current_batch_size + trial\n",
    "                loss, w_jacobian, b_jacobian = self.softmax_loss(X_train[i], y_train[i], penalty, alpha)\n",
    "                loss_batch -= loss; w_jacobian_batch += w_jacobian; b_jacobian_batch += b_jacobian\n",
    "            # average over the batch\n",
    "            loss_batch /= current_batch_size; w_jacobian_batch /= current_batch_size; b_jacobian_batch /= current_batch_size\n",
    "            self.gradient_descent(w_jacobian_batch, b_jacobian_batch, lr)\n",
    "            print(\"Epoch \" + str(batch_no+1) + \"\\a:\\a (loss: \" + str(loss_batch) + \")\")\n",
    "      \n",
    "    \n",
    "    \n",
    "    def softmax_loss(self, X, y, penalty=None, alpha=0.1):\n",
    "        \"\"\"\n",
    "            Compute the Loss and Gradients for a binary classifier\n",
    "            Return:\n",
    "                - Loss\n",
    "                - Gradient\n",
    "        \"\"\"\n",
    "        # 1. Compute the summ for softmax\n",
    "        summ = 0\n",
    "        for k in range(self.n_labels): # loop through the classes\n",
    "            w_k = self.weights[k]; b_k = self.biases[k]\n",
    "            summ += np.exp(np.dot(w_k, X) + b_k)\n",
    "        \n",
    "        # 2. Update the gradient matrixes, and calculate the loss\n",
    "        loss = 0; \n",
    "        w_jacobian = np.zeros(self.weights.shape); b_jacobian = np.zeros(self.biases.shape)\n",
    "        for k in range(self.n_labels):\n",
    "            w_k = self.weights[k]; b_k = self.biases[k]\n",
    "            softmax = (np.dot(w_k, X) + b_k) / summ # softmax\n",
    "            gradient = softmax - y[k]\n",
    "            for i in range(self.X_size):\n",
    "                w_jacobian[k][i] = gradient * X[i]\n",
    "                # Regularization\n",
    "                if (penalty == 'l1') : w_jacobian[k][i] -= alpha\n",
    "                elif (penalty == 'l2'): w_jacobian[k][i] -= alpha * 2 * w_k[i]\n",
    "            b_jacobian[k] = gradient\n",
    "            \n",
    "            # Loss -= softmax for the correct class\n",
    "            if (y[k] == 1): loss -= softmax;\n",
    "        \n",
    "        return loss, w_jacobian, b_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "1aa76282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\u0007:\u0007 (loss: 9.773776008198471e-05)\n",
      "Epoch 2\u0007:\u0007 (loss: 0.13501999737696854)\n",
      "Epoch 3\u0007:\u0007 (loss: 0.05011894826747039)\n",
      "Epoch 4\u0007:\u0007 (loss: 0.09421792525368791)\n",
      "Epoch 5\u0007:\u0007 (loss: 0.05274143069812355)\n",
      "Epoch 6\u0007:\u0007 (loss: 0.03856907933712221)\n",
      "Epoch 7\u0007:\u0007 (loss: 0.0235228454773977)\n",
      "Epoch 8\u0007:\u0007 (loss: 0.012031062242646513)\n",
      "Epoch 9\u0007:\u0007 (loss: 0.009401349297412775)\n",
      "Epoch 10\u0007:\u0007 (loss: 0.0066399147395609925)\n",
      "Epoch 11\u0007:\u0007 (loss: 0.0015094128467514657)\n",
      "Epoch 12\u0007:\u0007 (loss: 0.0005230919960064216)\n",
      "Epoch 13\u0007:\u0007 (loss: 0.0008406085808858963)\n",
      "Epoch 14\u0007:\u0007 (loss: 6.364405962713692e-05)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression2()\n",
    "classifier.fit(X_train, y_train, batch_size=10, penalty='l2',  alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c26dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP] *",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
